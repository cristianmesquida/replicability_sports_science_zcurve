@article{abt_rr2021,
  ids = {abtRegisteredReportsJournal2021b,abt_2021},
  title = {Registered Reports in the {{Journal}} of {{Sports Sciences}}},
  author = {Abt, Grant and Boreham, Colin and Davison, Gareth and Jackson, Robin and Wallace, Eric and Williams, A. Mark},
  year = 2021,
  volume = {39},
  number = {16},
  pages = {1789--1790},
  publisher = {Routledge},
  doi = {10.1080/02640414.2021.1950974},
  pmid = {34379576},
  file = {/Users/cristian/Zotero/storage/ULEKEUKR/Abt et al. - 2021 - Registered Reports in the Journal of Sports Scienc.pdf;/Users/cristian/Zotero/storage/5J22T76V/02640414.2021.html;/Users/cristian/Zotero/storage/ISW8PX65/02640414.2021.html;/Users/cristian/Zotero/storage/KUJWM3BN/02640414.2021.html;/Users/cristian/Zotero/storage/NWMVCUSS/02640414.2021.html}
}

@article{Abt2020,
  ids = {abt_2020},
  title = {Power, Precision, and Sample Size Estimation in Sport and Exercise Science Research},
  author = {Abt, Grant and Boreham, Colin and Davison, Gareth and Jackson, Robin and Nevill, Alan and Wallace, Eric and Williams, Mark},
  year = 2020,
  month = sep,
  journal = {Journal of Sports Sciences},
  volume = {38},
  number = {17},
  pages = {1933--1935},
  publisher = {Routledge},
  doi = {10.1080/02640414.2020.1776002},
  pmid = {32558628},
  file = {/Users/cristian/Zotero/storage/5HM4ISID/Abt et al. - 2020 - Power, precision, and sample size estimation in sp.pdf;/Users/cristian/Zotero/storage/BWHZA22W/02640414.2020.html;/Users/cristian/Zotero/storage/D4WEUIZ8/02640414.2020.html;/Users/cristian/Zotero/storage/I975XGPI/02640414.2020.html;/Users/cristian/Zotero/storage/KA29NBZD/02640414.2020.html;/Users/cristian/Zotero/storage/LTDUFQ7K/02640414.2020.html;/Users/cristian/Zotero/storage/QG9JSFLS/02640414.2020.html}
}

@article{bartos_2022,
  title = {Z-Curve 2.0: {{Estimating Replication Rates}} and {{Discovery Rates}}},
  shorttitle = {Z-Curve 2.0},
  author = {Barto{\v s}, Franti{\v s}ek and Schimmack, Ulrich},
  year = 2022,
  month = sep,
  journal = {Meta-Psychology},
  volume = {6},
  issn = {2003-2714},
  doi = {10.15626/MP.2021.2720},
  urldate = {2022-09-21},
  abstract = {Selection for statistical significance is a well-known factor that distorts the published literature and challenges the cumulative progress in science. Recent replication failures have fueled concerns that many published results are false-positives. Brunner and Schimmack (2020) developed z-curve, a method for estimating the expected replication rate (ERR) -- the predicted success rate of exact replication studies based on the mean power after selection for significance. This article introduces an extension of this method, z-curve 2.0. The main extension is an estimate of the expected discovery rate (EDR) -- the estimate of a proportion that the reported statistically significant results constitute from all conducted statistical tests. This information can be used to detect and quantify the amount of selection bias by comparing the EDR to the observed discovery rate (ODR; observed proportion of statistically significant results). In addition, we examined the performance of bootstrapped confidence intervals in simulation studies. Based on these results, we created robust confidence intervals with good coverage across a wide range of scenarios to provide information about the uncertainty in EDR and ERR estimates. We implemented the method in the zcurve R package (Barto{\v s} \&amp; Schimmack, 2020).},
  langid = {english},
  keywords = {Expected Discovery Rate,Expected Replication Rate,File-Drawer,Mixture Models,Power,Publication Bias,Selection Bias},
  file = {/Users/cristian/Zotero/storage/6NMSE9BC/Bartoš and Schimmack - 2022 - Z-curve 2.0 Estimating Replication Rates and Disc.pdf;/Users/cristian/Zotero/storage/UX6BWV36/Bartoš and Schimmack - 2022 - Z-curve 2.0 Estimating Replication Rates and Disc.pdf}
}

@article{bland_2011,
  title = {Comparisons against Baseline within Randomised Groups Are Often Used and Can Be Highly Misleading},
  author = {Bland, J Martin and Altman, Douglas G},
  year = 2011,
  month = dec,
  journal = {Trials},
  volume = {12},
  pages = {264},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-12-264},
  urldate = {2023-06-30},
  abstract = {Background In randomised trials, rather than comparing randomised groups directly some researchers carry out a significance test comparing a baseline with a final measurement separately in each group. Methods We give several examples where this has been done. We use simulation to demonstrate that the procedure is invalid and also show this algebraically. Results This approach is biased and invalid, producing conclusions which are, potentially, highly misleading. The actual alpha level of this procedure can be as high as 0.50 for two groups and 0.75 for three. Conclusions Randomised groups should be compared directly by two-sample methods and separate tests against baseline are highly misleading.},
  pmcid = {PMC3286439},
  pmid = {22192231},
  file = {/Users/cristian/Zotero/storage/RS8JECRF/Bland and Altman - 2011 - Comparisons against baseline within randomised gro.pdf}
}

@article{borg_2023,
  ids = {borgBiasStatisticalSignificance2023a},
  title = {The Bias for Statistical Significance in Sport and Exercise Medicine},
  author = {Borg, David N. and Barnett, Adrian G. and Caldwell, Aaron R. and White, Nicole M. and Stewart, Ian B.},
  year = 2023,
  month = mar,
  journal = {Journal of Science and Medicine in Sport},
  volume = {26},
  number = {3},
  pages = {164--168},
  issn = {1878-1861},
  doi = {10.1016/j.jsams.2023.03.002},
  abstract = {OBJECTIVES: We aimed to examine the bias for statistical significance using published confidence intervals in sport and exercise medicine research. DESIGN: Observational study. METHODS: The abstracts of 48,390 articles, published in 18 sports and exercise medicine journals between 2002 and 2022, were searched using a validated text-mining algorithm that identified and extracted ratio confidence intervals (odds, hazard, and risk ratios). The algorithm identified 1744 abstracts that included ratio confidence intervals, from which 4484 intervals were extracted. After excluding ineligible intervals, the analysis used 3819 intervals, reported as 95\,\% confidence intervals, from 1599 articles. The cumulative distributions of lower and upper confidence limits were plotted to identify any abnormal patterns, particularly around a ratio of 1 (the null hypothesis). The distributions were compared to those from unbiased reference data, which was not subjected to p-hacking or publication bias. A bias for statistical significance was further investigated using a histogram plot of z-values calculated from the extracted 95\,\% confidence intervals. RESULTS: There was a marked change in the cumulative distribution of lower and upper bound intervals just over and just under a ratio of 1. The bias for statistical significance was also clear in a stark under-representation of z-values between -1.96 and +1.96, corresponding to p-values above 0.05. CONCLUSIONS: There was an excess of published research with statistically significant results just below the standard significance threshold of 0.05, which is indicative of publication bias. Transparent research practices, including the use of registered reports, are needed to reduce the bias in published research.},
  langid = {english},
  pmid = {36966124},
  keywords = {-hacking,Bias,Confidence interval,Estimation,Exercise,Humans,Misconduct,Odds Ratio,p-hacking,Publication bias,Publication Bias,Selective reporting,Sports},
  file = {/Users/cristian/Zotero/storage/M9YHF69R/Borg et al. - 2023 - The bias for statistical significance in sport and.pdf;/Users/cristian/Zotero/storage/7ZP2IS9U/S1440244023000403.html;/Users/cristian/Zotero/storage/AAVWDDUN/S1440244023000403.html}
}

@article{brunner_2020,
  title = {Estimating {{Population Mean Power Under Conditions}} of {{Heterogeneity}} and {{Selection}} for {{Significance}}},
  author = {Brunner, Jerry and Schimmack, Ulrich},
  year = 2020,
  month = may,
  journal = {Meta-Psychology},
  volume = {4},
  issn = {2003-2714},
  doi = {10.15626/MP.2018.874},
  urldate = {2025-10-20},
  abstract = {In scientific fields that use significance tests, statistical power is important for successful replications of significant results because it is the long-run success rate in a series of exact replication studies. For any population of significant results, there is a population of power values of the statistical tests on which conclusions are based. We give exact theoretical results showing how selection for significance affects the distribution of statistical power in a heterogeneous population of significance tests. In a set of large-scale simulation studies, we compare four methods for estimating population mean power of a set of studies selected for significance (a maximum likelihood model, extensions of p-curve and p-uniform, \&amp; z-curve). The p-uniform and p-curve methods performed well with a fixed effects size and varying sample sizes. However, when there was substantial variability in effect sizes as well as sample sizes, both methods systematically overestimate mean power. With heterogeneity in effect sizes, the maximum likelihood model produced the most accurate estimates when the distribution of effect sizes matched the assumptions of the model, but z-curve produced more accurate estimates when the assumptions of the maximum likelihood model were not met. We recommend the use of z-curve to estimate the typical power of significant results, which has implications for the replicability of significant results in psychology journals.},
  copyright = {Copyright (c) 2020 Jerry Brunner, Ulrich Schimmack},
  langid = {english},
  keywords = {Effect size,Maximum likelihood,Meta-analysis,P-curve,P-uniform,Post-hoc power analysis,Power estimation,Publication bias,Replicability,Z-curve},
  file = {/Users/cristian/Zotero/storage/62XAR9DL/Brunner and Schimmack - 2020 - Estimating Population Mean Power Under Conditions of Heterogeneity and Selection for Significance.pdf}
}

@article{buttner2020,
  ids = {buttnerAreQuestionableResearch2020},
  title = {Are Questionable Research Practices Facilitating New Discoveries in Sport and Exercise Medicine? {{The}} Proportion of Supported Hypotheses Is Implausibly High},
  author = {B{\"u}ttner, Fionn and Toomey, Elaine and McClean, Shane and Roe, Mark and Delahunt, Eamonn},
  year = 2020,
  journal = {British Journal of Sports Medicine},
  volume = {54},
  number = {22},
  doi = {10.1136/bjsports-2019-101863},
  pages = {1365--1371},
  isbn = {0306-3674},
  pmid = {32699001},
  keywords = {education,methodological,research,sport,statistics}
}

@article{button2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = 2013,
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  doi = {10.1038/nrn3475},
  abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between {$\sim$}8\% and {$\sim$}31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/5QQNF2JE/Button et al. - 2013 - Power failure why small sample size undermines th.pdf;/Users/cristian/Zotero/storage/QZXNHZZX/nrn3475.html}
}

@article{chambers_2021,
  title = {The Past, Present and Future of {{Registered Reports}}},
  author = {Chambers, Christopher D. and Tzavella, Loukia},
  year = 2021,
  month = nov,
  journal = {Nature Human Behaviour},
  pages = {1--14},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01193-7},
  urldate = {2022-01-13},
  abstract = {Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  keywords = {Culture,Publishing},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Reviews\\
Subject\_term: Culture;Publishing\\
Subject\_term\_id: culture;publishing},
  file = {/Users/cristian/Zotero/storage/LXLPGUE4/Chambers and Tzavella - 2021 - The past, present and future of Registered Reports.pdf;/Users/cristian/Zotero/storage/HQ5LDFGB/s41562-021-01193-7.html}
}

@article{colquhoun2014,
  title = {An Investigation of the False Discovery Rate and the Misinterpretation of P-Values},
  author = {Colquhoun, David},
  year = 2014,
  month = nov,
  journal = {Royal Society Open Science},
  volume = {1},
  number = {3},
  pages = {140216},
  publisher = {Royal Society},
  doi = {10.1098/rsos.140216},
  urldate = {2024-11-05},
  abstract = {If you use p=0.05 to suggest that you have made a discovery, you will be wrong at least 30\% of the time. If, as is often the case, experiments are underpowered, you will be wrong most of the time. This conclusion is demonstrated from several points of view. First, tree diagrams which show the close analogy with the screening test problem. Similar conclusions are drawn by repeated simulations of t-tests. These mimic what is done in real life, which makes the results more persuasive. The simulation method is used also to evaluate the extent to which effect sizes are over-estimated, especially in underpowered experiments. A script is supplied to allow the reader to do simulations themselves, with numbers appropriate for their own work. It is concluded that if you wish to keep your false discovery rate below 5\%, you need to use a three-sigma rule, or to insist on p{$\leq$}0.001. And never use the word `significant'.},
  keywords = {false discovery rate,reproducibility,significance tests,statistics},
  file = {/Users/cristian/Zotero/storage/W3TC5WE9/Colquhoun - 2014 - An investigation of the false discovery rate and t.pdf}
}

@article{curran2009,
  title = {The Seemingly Quixotic Pursuit of a Cumulative Psychological Science: {{Introduction}} to the Special Issue},
  shorttitle = {The Seemingly Quixotic Pursuit of a Cumulative Psychological Science},
  author = {Curran, Patrick J.},
  year = 2009,
  journal = {Psychological Methods},
  volume = {14},
  number = {2},
  pages = {77--80},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/a0015972},
  abstract = {The goal of any empirical science is to pursue the construction of a cumulative base of knowledge upon which the future of the science may be built. However, there is mixed evidence that the science of psychology can accurately be characterized by such a cumulative progression. Indeed, some argue that the development of a truly cumulative psychological science is not possible with the current paradigms of hypothesis testing in single-study designs. The author explores this controversy as a framework to introduce the 6 articles that make up this special issue on the integration of data and empirical findings across multiple studies. The author proposes that the methods and techniques described in this set of articles can significantly propel researchers forward in their ongoing quest to build a cumulative psychological science. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Hypothesis Testing,Meta Analysis,Psychology,Statistical Analysis,Statistical Data},
  file = {/Users/cristian/Zotero/storage/Y85ZRLBW/Curran - 2009 - The seemingly quixotic pursuit of a cumulative psy.pdf;/Users/cristian/Zotero/storage/XAHX7GKE/2009-08072-006.html}
}

@article{errington2021,
  title = {Investigating the Replicability of Preclinical Cancer Biology},
  author = {Errington, Timothy M. and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
  editor = {Pasqualini, Renata and Franco, Eduardo},
  year = 2021,
  month = dec,
  journal = {eLife},
  volume = {10},
  pages = {e71601},
  doi = {10.7554/eLife.71601},
  abstract = {Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85\% smaller than the median effect size in the original experiments, and 92\% of replication effect sizes were smaller than the original. The other methods were binary -- the replication was either a success or a failure -- and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40\% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80\% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46\% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.},
  keywords = {credibility,meta-analysis,replication,reproducibility,reproducibility in cancer biology,Reproducibility Project: Cancer Biology,transparency},
  file = {/Users/cristian/Zotero/storage/3EYQZQ85/Errington et al. - 2021 - Investigating the replicability of preclinical can.pdf}
}

@article{impellizzeri_2019,
  title = {Registered Reports Coming Soon: Our Contribution to Better Science in Football Research},
  author = {Impellizzeri, Franco M. and McCall, Alan and Meyer, Tim},
  year = 2019,
  journal = {Science and Medicine in Football},
  volume = {3},
  number = {2},
  pages = {87--88},
  doi = {10.1080/24733938.2019.1603659},
  keywords = {Bias,Good practice,Quality,Registration,Science},
  file = {/Users/cristian/Zotero/storage/F5SJWYCC/Impellizzeri et al. - 2019 - Registered reports coming soon our contribution t.pdf;/Users/cristian/Zotero/storage/7KQP3YNY/24733938.2019.html}
}

@article{lakens_2024,
  title = {The Benefits of Preregistration and {{Registered Reports}}},
  author = {Lakens, Dani{\"e}l and Mesquida, Cristian and Rasti, Sajedeh and Ditroilo, Massimiliano},
  year = 2024,
  month = dec,
  journal = {Evidence-Based Toxicology},
  volume = {2},
  number = {1},
  pages = {2376046},
  issn = {2833-373X},
  doi = {10.1080/2833373X.2024.2376046},
  urldate = {2025-02-17},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/IQDT6CXN/Lakens et al. - 2024 - The benefits of preregistration and Registered Rep.pdf}
}

@article{lakens_equivalence_2017,
  title = {Equivalence {{Tests}}: {{A Practical Primer}} for t {{Tests}}, {{Correlations}}, and {{Meta-Analyses}}},
  author = {Lakens, Dani{\"e}l},
  year = 2017,
  month = may,
  journal = {Social Psychological and Personality Science},
  doi = {10.1177/1948550617697177},
  abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
  langid = {english},
  keywords = {equivalence testing,null hypothesis significance testing,power analysis,research methods},
  file = {/Users/cristian/Zotero/storage/PZC95BMI/Lakens - 2017 - Equivalence Tests A Practical Primer for t Tests,.pdf}
}

@article{lakenssamplejustification,
  ids = {lakens_justification_2022},
  title = {Sample {{Size Justification}}},
  author = {Lakens, Dani{\"e}l},
  year = 2022,
  month = mar,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {33267},
  issn = {2474-7394},
  doi = {10.1525/collabra.33267},
  urldate = {2022-10-17},
  abstract = {An important step when designing an empirical study is to justify the sample size that will be collected. The key aim of a sample size justification for such studies is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are expected in a specific research area. Researchers can use the guidelines presented in this article, for example by using the interactive form in the accompanying online Shiny app, to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
  file = {/Users/cristian/Zotero/storage/ICZW3DHQ/Lakens - 2022 - Sample Size Justification.pdf;/Users/cristian/Zotero/storage/WHLG9TBD/Sample-Size-Justification.html}
}

@article{mahoney1977,
  title = {Publication Prejudices: {{An}} Experimental Study of Confirmatory Bias in the Peer Review System},
  author = {Mahoney, Michael J.},
  year = 1977,
  month = jun,
  journal = {Cognitive Therapy and Research},
  volume = {1},
  pages = {161--175},
  doi = {10.1007/BF01173636},
  abstract = {Confirmatory bias is the tendency to emphasize and believe experiences which support one's views and to ignore or discredit those which do not. The effects of this tendency have been repeatedly documented in clinical research. However, its ramifications for the behavior of scientists have yet to be adequately explored. For example, although publication is a critical element in determining the contribution and impact of scientific findings, little research attention has been devoted to the variables operative in journal review policies. In the present study, 75 journal reviewers were asked to referee manuscripts which described identical experimental procedures but which reported positive, negative, mixed, or no results. In addition to showing poor interrater agreement, reviewers were strongly biased against manuscripts which reported results contrary to their theoretical perspective. The implications of these findings for epistemology and the peer review system are briefly addressed.},
  langid = {english}
}

@article{maxwell2004,
  title = {The {{Persistence}} of {{Underpowered Studies}} in {{Psychological Research}}: {{Causes}}, {{Consequences}}, and {{Remedies}}},
  author = {Maxwell, Scott E.},
  year = 2004,
  journal = {Psychological Methods},
  volume = {9},
  number = {2},
  pages = {147--163},
  doi = {10.1037/1082-989X.9.2.147},
  abstract = {Underpowered studies persist in the psychological literature. This article examines reasons for their persistence and the effects on efforts to create a cumulative science. The "curse of multiplicities" plays a central role in the presentation. Most psychologists realize that testing multiple hypotheses in a single study affects the Type I error rate, but corresponding implications for power have largely been ignored. The presence of multiple hypothesis tests leads to 3 different conceptualizations of power. Implications of these 3 conceptualizations are discussed from the perspective of the individual researcher and from the perspective of developing a coherent literature. Supplementing significance tests with effect size measures and confidence intervals is shown to address some but not necessarily all problems associated with multiple testing. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Confidence Limits (Statistics),Consequence,Effect Size (Statistical),Hypothesis Testing,Methodology,Psychology,Statistical Power,Type I Errors},
  file = {/Users/cristian/Zotero/storage/ZIE2UQD5/Maxwell - 2004 - The Persistence of Underpowered Studies in Psychol.pdf;/Users/cristian/Zotero/storage/P35K8BIW/2004-14114-001.html}
}

@article{mesquida2022,
  title = {Replication Concerns in Sports and Exercise Science: A Narrative Review of Selected Methodological Issues in the Field},
  shorttitle = {Replication Concerns in Sports and Exercise Science},
  author = {Mesquida, Cristian and Murphy, Jennifer and Lakens, Dani{\"e}l and Warne, Joe},
  year = 2022,
  month = dec,
  journal = {Royal Society Open Science},
  volume = {9},
  number = {12},
  pages = {220946},
  publisher = {Royal Society},
  doi = {10.1098/rsos.220946},
  urldate = {2025-03-29},
  abstract = {Known methodological issues such as publication bias, questionable research practices and studies with underpowered designs are known to decrease the replicability of study findings. The presence of such issues has been widely established across different research fields, especially in psychology. Their presence raised the first concerns that the replicability of study findings could be low and led researchers to conduct large replication projects. These replication projects revealed that a significant portion of original study findings could not be replicated, giving rise to the conceptualization of the replication crisis. Although previous research in the field of sports and exercise science has identified the first warning signs, such as an overwhelming proportion of significant findings, small sample sizes and lack of data availability, their possible consequences for the replicability of our field have been overlooked. We discuss the consequences of the above issues on the replicability of our field and offer potential solutions to improve replicability.},
  keywords = {hypothesis testing,Open Science practices,publication bias,questionable research practices,replicability,statistical power},
  file = {/Users/cristian/Zotero/storage/P72A75KI/Mesquida et al. - 2022 - Replication concerns in sports and exercise scienc.pdf}
}

@article{mesquida_2023,
  title = {Publication Bias, Statistical Power and Reporting Practices in the {{Journal}} of {{Sports Sciences}}: Potential Barriers to Replicability},
  shorttitle = {Publication Bias, Statistical Power and Reporting Practices in the {{Journal}} of {{Sports Sciences}}},
  author = {Mesquida, Cristian and Murphy, Jennifer and Lakens, Dani{\"e}l and Warne, Joe},
  year = 2023,
  month = sep,
  journal = {Journal of Sports Sciences},
  volume = {41},
  number = {16},
  pages = {1507--1517},
  issn = {1466-447X},
  doi = {10.1080/02640414.2023.2269357},
  abstract = {Two factors that decrease the replicability of studies in the scientific literature are publication bias and studies with underpowered desgins. One way to ensure that studies have adequate statistical power to detect the effect size of interest is by conducting a-priori power analyses. Yet, a previous editorial published in the Journal of Sports Sciences reported a median sample size of 19 and the scarce usage of a-priori power analyses. We meta-analysed 89 studies from the same journal to assess the presence and extent of publication bias, as well as the average statistical power, by conducting a z-curve analysis. In a larger sample of 174 studies, we also examined a) the usage, reporting practices and reproducibility of a-priori power analyses; and b) the prevalence of reporting practices of t-statistic or F-ratio, degrees of freedom, exact p-values, effect sizes and confidence intervals. Our results indicate that there was some indication of publication bias and the average observed power was low (53\% for significant and non-significant findings and 61\% for only significant findings). Finally, the usage and reporting practices of a-priori power analyses as well as statistical results including test statistics, effect sizes and confidence intervals were suboptimal.},
  langid = {english},
  pmid = {38018365},
  keywords = {Bias,Humans,publication bias,Publication Bias,Replicability,reporting practices,reproducibility,Reproducibility of Results,Research Design,Sample Size,statistical power},
  file = {/Users/cristian/Zotero/storage/WNA9JV57/Mesquida et al. - 2023 - Publication bias, statistical power and reporting .pdf}
}

@misc{mesquida_power_analysis_2025,
  title = {The Prevalence, Reporting Practices, and Methodological Quality of a Priori Power Analyses in Sports and Exercise Science Research},
  author = {Mesquida, Cristian and Murphy, Jennifer and Warne, Joe and Lakens, Dani{\"e}l},
  year = 2025,
  month = jul,
  publisher = {SportRxiv},
  doi = {10.51224/SRXIV.575},
  urldate = {2025-07-03},
  archiveprefix = {SportRxiv},
  langid = {english},
  keywords = {reproducibility,sample size,statistical power},
  file = {/Users/cristian/Zotero/storage/N9K58KDV/Mesquida et al. - 2025 - The prevalence, reporting practices, and methodolo.pdf}
}

@article{murphy_2022,
  title = {Proposal of a {{Selection Protocol}} for {{Replication}} of {{Studies}} in {{Sports}} and {{Exercise Science}}},
  author = {Murphy, Jennifer and Mesquida, Cristian and Caldwell, Aaron R. and Earp, Brian D. and Warne, Joe P.},
  year = 2022,
  month = sep,
  journal = {Sports Medicine},
  issn = {1179-2035},
  doi = {10.1007/s40279-022-01749-1},
  abstract = {To improve the rigor of science, experimental evidence for scientific claims ideally needs to be replicated repeatedly with comparable analyses and new data to increase the collective confidence in the veracity of those claims. Large replication projects in psychology and cancer biology have evaluated the replicability of their fields but no collaborative effort has been undertaken in sports and exercise science. We propose to undertake such an effort here. As this is the first large replication project in this field, there is no agreed-upon protocol for selecting studies to replicate. Criticism of previous selection protocols include claims they were non-randomised and non-representative. Any selection protocol in sports and exercise science must be representative to provide an accurate estimate of replicability of the field. Our aim is to produce a protocol for selecting studies to replicate for inclusion in a large replication project in sports and exercise science. METHODS: The proposed selection protocol uses multiple inclusion and exclusion criteria for replication study selection, including: the year of publication and citation rankings, research disciplines, study types, the research question and key dependent variable, study methods and feasibility. Studies selected for replication will be stratified into pools based on instrumentation and expertise required, and will then be allocated to volunteer laboratories for replication. Replication outcomes will be assessed using a multiple inferential strategy and descriptive information will be reported regarding the final number of included and excluded studies, and original author responses to requests for raw data.},
  langid = {english},
  pmid = {36066754},
  file = {/Users/cristian/Zotero/storage/IEKPQ28N/Murphy et al. - 2022 - Proposal of a Selection Protocol for Replication o.pdf}
}

@article{murphy_replicability_2025,
  title = {Estimating the {{Replicability}} of {{Sports}} and {{Exercise Science Research}}},
  author = {Murphy, Jennifer and Caldwell, Aaron R. and Mesquida, Cristian and Ladell, Aera J. M. and {Encarnaci{\'o}n-Mart{\'i}nez}, Alberto and Tual, Alexandre and Denys, Andrew and Cameron, Bailey and Van Hooren, Bas and Parr, Ben and DeLucia, Bianca and Mason, Billy R. J. and Clark, Brad and Egan, Brendan and Brown, Calum and Ade, Carl and Sforza, Chiarella and Taber, Christopher B. and Kirk, Christopher and McCrum, Christopher and Tighe, Cian Okeeffe and Byrne, Ciara and Brunetti, Claudia and Forestier, Cyril and Martin, Dan and Taylor, Danny and Diggin, David and Gallagher, Dearbhla and King, Deborah L. and Rogers, Elizabeth and Bennett, Eric C. and Lopatofsky, Eric T. and Dunn, Gemma and Gauchar, G{\'e}rome C. and Mornieux, Guillaume and {Catal{\'a}-Vilaplana}, Ignacio and Caetan, Ines and {Aparicio-Aparicio}, Inmaculada and Barnes, Jack and Blaisdell, Jake and Steele, James and Fletcher, Jared R. and Hutchinson, Jasmin and Au, Jason and Oliemans, Jason P. and Bakhshinejad, Javad and Barrios, Joaquin and Quesada, Jose Ignacio Priego and Rager, Joseph and Capon, Julia B. and Walton, Julie S. J. and Stevens, Kailey and Heinrich, Katie and Wu, Kelly and Meijer, Kenneth and Richards, Laura and Jutlah, Lauren and Tong, Le and Bridgeman, Lee and Banet, Leo and Mbiyu, Leonard and Sefton, Lucy and {de Chanaleilles}, Margaret and Charisi, Maria and Beerse, Matthew and Major, Matthew J. and Caon, Maya and Bargh, Mel and Rowley, Michael and Moran, Miguel Vaca and Croker, Nicholas and Hanen, Nicolas C. and Montague, Nicole and Brick, Noel E. and Runswick, Oliver R. and Willems, Paul and {P{\'e}rez-Soriano}, Pedro and Blake, Rebecca and Jones, Rebecca and Quinn, Rebecca Louise and {Sanchis-Sanchis}, Roberto and Rabello, Rodrigo and Bolger, Roisin and Shohat, Roy and Cotton, Sadie and Chua, Samantha and Norwood, Samuel and Vimeau, Samuel and Dias, Sandro and Pedersen, Sissel and Skaper, Spencer S. and Coyle, Taylor and Desai, Terun and Gee, Thomas I. and Edwards, Tobias and Pohl, Torsten and Yingling, Vanessa and Ribeiro, Vinicius and Duchene, Youri and Papadakis, Zacharias and Warne, Joe P.},
  year = 2025,
  journal = {Sports Medicine},
  issn = {1179-2035},
  doi = {10.1007/s40279-025-02201-w},
  urldate = {2025-06-17},
  abstract = {The replicability of sports and exercise research has not been assessed previously despite concerns about scientific practices within the field.},
  langid = {english},
  keywords = {Communication and replication,Experimental Psychology,Psychometrics,Sport Psychology,Sport Science,Sport Theory},
  file = {/Users/cristian/Zotero/storage/JWDTUMR6/Murphy et al. - 2025 - Estimating the Replicability of Sports and Exercis.pdf}
}

@article{nosek_lakens_2014,
  title = {Registered Reports: {{A}} Method to Increase the Credibility of Published Results.},
  author = {Nosek, Brian A. and Lakens, Dani{\"e}l},
  year = 2014,
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  doi = {10.1027/1864-9335/a000192},
  file = {/Users/cristian/Zotero/storage/A8XSDE67/Nosek and Lakens - Registered reports A method to increase the credi.pdf;/Users/cristian/Zotero/storage/T5JFHB4E/2014-20922-001.html}
}

@article{osc2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = 2015,
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  issn = {1095-9203},
  doi = {10.1126/science.aac4716},
  abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  langid = {english},
  pmid = {26315443},
  keywords = {Behavioral Research,Confidence Intervals,Periodicals as Topic,Psychology,Publication Bias,Publishing,Reproducibility of Results,Research,Research Design},
  file = {/Users/cristian/Zotero/storage/SC9BGVNR/Open Science Collaboration - 2015 - PSYCHOLOGY. Estimating the reproducibility of psyc.pdf}
}

@article{quintana_2020,
  title = {Most Oxytocin Administration Studies Are Statistically Underpowered to Reliably Detect (or Reject) a Wide Range of Effect Sizes},
  author = {Quintana, Daniel S.},
  year = 2020,
  month = nov,
  journal = {Comprehensive Psychoneuroendocrinology},
  volume = {4},
  pages = {100014},
  issn = {2666-4976},
  doi = {10.1016/j.cpnec.2020.100014},
  abstract = {The neuropeptide oxytocin has attracted substantial research interest for its role in behaviour and cognition; however, the evidence for its effects have been mixed. Meta-analysis is viewed as the gold-standard for synthesizing evidence, but the evidential value of a meta-analysis is dependent on the evidential value of the studies it synthesizes, and the analytical approaches used to derive conclusions. To assess the evidential value of oxytocin administration meta-analyses, this study calculated the statistical power of 107 studies from 35 meta-analyses and assessed the statistical equivalence of reported results. The mean statistical power across all studies was 12.2\% and there has been no noticeable improvement in power over an eight-year period. None of the 26 non-significant meta-analyses were statistically equivalent, assuming a smallest effect size of interest of 0.1. Altogether, most oxytocin treatment study designs are statistically underpowered to either detect or reject a wide range of worthwhile effect sizes.},
  langid = {english},
  pmcid = {PMC9216440},
  pmid = {35755627},
  keywords = {Neuroendocrinology,Oxytocin,Social behaviour,Statistics},
  file = {/Users/cristian/Zotero/storage/CCWHE2GZ/Quintana - 2020 - Most oxytocin administration studies are statistic.pdf}
}

@article{rosenthal1979,
  title = {The File Drawer Problem and Tolerance for Null Results},
  author = {Rosenthal, Robert},
  year = 1979,
  journal = {Psychological Bulletin},
  volume = {83},
  number = {3},
  pages = {638--641},
  doi = {10.1037/0033-2909.86.3.638},
  abstract = {For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type I errors, while the file drawers are filled with the 95\% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimentation,Scientific Communication,Statistical Probability,Statistical Tests,Type I Errors},
  file = {/Users/cristian/Zotero/storage/LA56HT4M/1979-27602-001.html}
}

@article{scheel2022,
  title = {An {{Excess}} of {{Positive Results}}: {{Comparing}} the {{Standard Psychology Literature With Registered Reports}}},
  shorttitle = {An {{Excess}} of {{Positive Results}}},
  author = {Scheel, Anne M. and Schijen, Mitchell R. M. J. and Lakens, Dani{\"e}l},
  year = 2021,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {1--12},
  doi = {10.1177/25152459211007467},
  abstract = {Selectively publishing results that support the tested hypotheses (``positive'' results) distorts the available evidence for scientific claims. For the past decade, psychological scientists have been increasingly concerned about the degree of such distortion in their literature. A new publication format has been developed to prevent selective reporting: In Registered Reports (RRs), peer review and the decision to publish take place before results are known. We compared the results in published RRs (N = 71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature (N = 152) in psychology. Analyzing the first hypothesis of each article, we found 96\% positive results in standard reports but only 44\% positive results in RRs. We discuss possible explanations for this large difference and suggest that a plausible factor is the reduction of publication bias and/or Type I error inflation in the RR literature.},
  langid = {english},
  keywords = {hypothesis testing,open data,preregistered,publication bias,Registered Reports},
  annotation = {https://doi.org/10.1177/25152459211007467},
  file = {/Users/cristian/Zotero/storage/5DFUNCJU/Scheel et al. - 2021 - An Excess of Positive Results Comparing the Stand.pdf;/Users/cristian/Zotero/storage/EHD3RBMW/Scheel et al. - 2021 - An Excess of Positive Results Comparing the Stand.pdf}
}

@article{stefan2023,
  ids = {stefanBigLittleLies2022},
  title = {Big Little Lies: A Compendium and Simulation of p-Hacking Strategies},
  shorttitle = {Big Little Lies},
  author = {Stefan, A. M. and Sch{\"o}nbrodt, Felix D.},
  year = 2023,
  month = feb,
  journal = {Royal Society Open Science},
  volume = {10},
  number = {2},
  pages = {220346},
  publisher = {Royal Society},
  doi = {10.1098/rsos.220346},
  urldate = {2023-02-13},
  abstract = {In many research fields, the widespread use of questionable research practices has jeopardized the credibility of scientific results. One of the most prominent questionable research practices is p-hacking. Typically, p-hacking is defined as a compound of strategies targeted at rendering non-significant hypothesis testing results significant. However, a comprehensive overview of these p-hacking strategies is missing, and current meta-scientific research often ignores the heterogeneity of strategies. Here, we compile a list of 12 p-hacking strategies based on an extensive literature review, identify factors that control their level of severity, and demonstrate their impact on false-positive rates using simulation studies. We also use our simulation results to evaluate several approaches that have been proposed to mitigate the influence of questionable research practices. Our results show that investigating p-hacking at the level of strategies can provide a better understanding of the process of p-hacking, as well as a broader basis for developing effective countermeasures. By making our analyses available through a Shiny app and R package, we facilitate future meta-scientific research aimed at investigating the ramifications of p-hacking across multiple strategies, and we hope to start a broader discussion about different manifestations of p-hacking in practice.},
  keywords = {error rates,Error Rates,False Positive Results,false-positive rate,Meta-science,p-curve,p-Curve,Quantitative Methods,questionable research practices,Questionable Research Practices,Shiny app,Shiny App,significance,Significance,simulation,Simulation,Social and Behavioral Sciences,Statistical Methods},
  file = {/Users/cristian/Zotero/storage/P5IJHMR6/Stefan and Schönbrodt - 2022 - Big Little Lies A Compendium and Simulation of p-.pdf;/Users/cristian/Zotero/storage/X7DJUPS5/Stefan and Schönbrodt - 2023 - Big little lies a compendium and simulation of p-.pdf}
}

@article{sterling1995,
  title = {Publication {{Decisions Revisited}}: {{The Effect}} of the {{Outcome}} of {{Statistical Tests}} on the {{Decision}} to {{Publish}} and {{Vice Versa}}},
  author = {Sterling, T. D. and Rosenbaum, W. L. and Weinkam, J. J.},
  year = 1995,
  journal = {The American Statistician},
  volume = {49},
  number = {1},
  pages = {108--112},
  doi = {10.2307/2684823},
  abstract = {This article presents evidence that published results of scientific investigations are not a representative sample of results of all scientific studies. Research studies from 11 major journals demonstrate the existence of biases that favor studies that observe effects that, on statistical evaluation, have a low probability of erroneously rejecting the so-called null hypothesis (H 0). This practice makes the probability of erroneously rejecting H 0 different for the reader than for the investigator. It introduces two biases in the interpretation of the scientific literature: one due to multiple repetition of studies with false hypothesis, and one due to failure to publish smaller and less significant outcomes of tests of a true hypotheses. These practices distort the results of literature surveys and of meta-analyses. These results also indicate that practice leading to publication bias have not changed over a period of 30 years.},
  keywords = {Bias,Null results,Publication bias,Tests of significance},
  annotation = {https://doi.org/10.2307/2684823},
  file = {/Users/cristian/Zotero/storage/TVPKKJZX/00031305.1995.html}
}

@article{szucs2017,
  title = {Empirical Assessment of Published Effect Sizes and Power in the Recent Cognitive Neuroscience and Psychology Literature},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  year = 2017,
  month = mar,
  journal = {PLoS Biology},
  volume = {19},
  number = {3},
  pages = {e3001151},
  doi = {10.1371/journal.pbio.2000797},
  abstract = {We have empirically assessed the distribution of published effect sizes and estimated power by analyzing 26,841 statistical records from 3,801 cognitive neuroscience and psychology papers published recently. The reported median effect size was D = 0.93 (interquartile range: 0.64--1.46) for nominally statistically significant results and D = 0.24 (0.11--0.42) for nonsignificant results. Median power to detect small, medium, and large effects was 0.12, 0.44, and 0.73, reflecting no improvement through the past half-century. This is so because sample sizes have remained small. Assuming similar true effect sizes in both disciplines, power was lower in cognitive neuroscience than in psychology. Journal impact factors negatively correlated with power. Assuming a realistic range of prior probabilities for null hypotheses, false report probability is likely to exceed 50\% for the whole literature. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience., Biomedical science, psychology, and many other fields may be suffering from a serious replication crisis. In order to gain insight into some factors behind this crisis, we have analyzed statistical information extracted from thousands of cognitive neuroscience and psychology research papers. We established that the statistical power to discover existing relationships has not improved during the past half century. A consequence of low statistical power is that research studies are likely to report many false positive findings. Using our large dataset, we estimated the probability that a statistically significant finding is false (called false report probability). With some reasonable assumptions about how often researchers come up with correct hypotheses, we conclude that more than 50\% of published findings deemed to be statistically significant are likely to be false. We also observed that cognitive neuroscience studies had higher false report probability than psychology studies, due to smaller sample sizes in cognitive neuroscience. In addition, the higher the impact factors of the journals in which the studies were published, the lower was the statistical power. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience.},
  file = {/Users/cristian/Zotero/storage/BW987VQP/Szucs and Ioannidis - 2017 - Empirical assessment of published effect sizes and.pdf}
}

@article{twomey_2021,
  title = {The {{Nature}} of {{Our Literature}}: {{A Registered Report}} on the {{Positive Result Rate}} and {{Reporting Practices}} in {{Kinesiology}}},
  author = {Twomey, Rosie and Yingling, Vanessa and Warne, Joe and Schneider, Christoph and McCrum, Christopher and Atkins, Whitley and Murphy, Jennifer and Medina, Claudia Romero and Harlley, Sena and Caldwell, Aaron},
  year = 2021,
  month = dec,
  journal = {Communications in Kinesiology},
  volume = {1},
  number = {3},
  pages = {1--17},
  doi = {10.51224/cik.v1i3.43},
  abstract = {Scientists rely upon an accurate scientific literature in order to build and test new theories about the natural world. In the past decade, observational studies of the scientific literature have indicated that numerous questionable research practices and poor reporting practices may be hindering scientific progress. In particular, 3 recent studies have indicated an implausibly high rate of studies with positive (i.e., hypothesis confirming) results. In sports medicine, a field closely related to kinesiology, studies that tested a hypothesis indicated support for their primary hypothesis {\textasciitilde}70\% of the time. However, a study of journals that cover the entire field of kinesiology has yet to be completed, and the quality of other reporting practices, such as clinical trial registration, has not been evaluated. In this study we retrospectively evaluated 300 original research articles from the flagship journals of North America (Medicine and Science in Sports and Exercise), Europe (European Journal of Sport Science), and Australia (Journal of Science and Medicine in Sport). The hypothesis testing rate ({\textasciitilde}64\%) and positive result rate ({\textasciitilde}81\%) were much lower than what has been reported in other fields (e.g., psychology), and there was only weak evidence for our hypothesis that the positive result rate exceeded 80\%. However, the positive result rate is still considered unreasonably high. Additionally, most studies did not report trial registration, and rarely included accessible data indicating rather poor reporting practices. The majority of studies relied upon significance testing ({\textasciitilde}92\%), but it was more concerning that a majority of studies ({\textasciitilde}82\%) without a stated hypothesis still relied upon significance testing. Overall, the positive result rate in kinesiology is unacceptably high, despite being lower than other fields such as psychology, and most published manuscripts demonstrated subpar reporting practices},
  langid = {english},
  keywords = {sport and exercise science},
  annotation = {https://doi.org/10.51224/cik.v1i3.43},
  file = {/Users/cristian/Zotero/storage/XCTLFNDG/Twomey et al. - 2021 - The Nature of Our Literature A Registered Report .pdf}
}

@article{wicherts_2016,
  title = {Degrees of {{Freedom}} in {{Planning}}, {{Running}}, {{Analyzing}}, and {{Reporting Psychological Studies}}: {{A Checklist}} to {{Avoid}} p-{{Hacking}}},
  author = {Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Augusteijn, Hilde E. M. and Bakker, Marjan and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M.},
  year = 2016,
  month = nov,
  journal = {Frontiers in Psychology},
  volume = {7},
  pages = {1832},
  doi = {10.3389/fpsyg.2016.01832},
  abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.},
  file = {/Users/cristian/Zotero/storage/ZSV6HSSE/Wicherts et al. - 2016 - Degrees of Freedom in Planning, Running, Analyzing.pdf}
}

@article{wilson2018,
  title = {The {{Prior Odds}} of {{Testing}} a {{True Effect}} in {{Cognitive}} and {{Social Psychology}}},
  author = {Wilson, Brent M. and Wixted, John T.},
  year = 2018,
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {186--197},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245918767122},
  urldate = {2023-01-13},
  abstract = {Efforts to increase replication rates in psychology generally consist of recommended improvements to methodology, such as increasing sample sizes to increase power or using a lower alpha level. However, little attention has been paid to how the prior odds (R) that a tested effect is true can affect the probability that a significant result will be replicable. The lower R is, the less likely a published result will be replicable even if power is high. It follows that if R is lower in one set of studies than in another, then all else being equal, published results will be less replicable in the set with lower R. We illustrate this point by presenting an analysis of data from the social-psychology and cognitive-psychology studies that were included in the Open Science Collaboration?s (2015) replication project. We found that R was lower for the social-psychology studies than for the cognitive-psychology studies, which might explain why the rate of successful replications differed between these two sets of studies. This difference in replication rates may reflect the degree to which scientists in the two fields value risky but potentially groundbreaking (i.e., low-R) research. Critically, high-R research is not inherently better or worse than low-R research for advancing knowledge. However, if they wish to achieve replication rates comparable to those of high-R fields (a judgment call), researchers in low-R fields would need to use an especially low alpha level, conduct experiments that have especially high power, or both.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/UE8HD3XX/Wilson and Wixted - 2018 - The Prior Odds of Testing a True Effect in Cogniti.pdf}
}

@Misc{zcurve_pkg,
    title = {zcurve: An R Package for Fitting Z-curves},
    author = {František Bartoš and Ulrich Schimmack},
    year = {2020},
    note = {R package version 2.4.2},
    url = {https://CRAN.R-project.org/package=zcurve},
  }
  
@Manual{Rcoreteam,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2024},
    url = {https://www.R-project.org/},
  }

@Manual{quarto_pkg,
    title = {quarto: R Interface to 'Quarto' Markdown Publishing System},
    author = {JJ Allaire and Christophe Dervieux},
    year = {2025},
    note = {R package version 1.5.1},
    url = {https://CRAN.R-project.org/package=quarto},
  }

@Book{ggplot2_pkg,
    author = {Hadley Wickham},
    title = {ggplot2: Elegant Graphics for Data Analysis},
    publisher = {Springer-Verlag New York},
    year = {2016},
    isbn = {978-3-319-24277-4},
    url = {https://ggplot2.tidyverse.org},
  }

 @Manual{dplyr_pkg,
    title = {dplyr: A Grammar of Data Manipulation},
    author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller and Davis Vaughan},
    year = {2023},
    note = {R package version 1.1.4},
    url = {https://CRAN.R-project.org/package=dplyr},
  }

@Manual{readxl_pkg,
    title = {readxl: Read Excel Files},
    author = {Hadley Wickham and Jennifer Bryan},
    year = {2025},
    note = {R package version 1.4.5},
    url = {https://CRAN.R-project.org/package=readxl},
  }

@InCollection{knitr_pkg,
    booktitle = {Implementing Reproducible Computational Research},
    editor = {Victoria Stodden and Friedrich Leisch and Roger D. Peng},
    title = {knitr: A Comprehensive Tool for Reproducible Research in {R}},
    author = {Yihui Xie},
    publisher = {Chapman and Hall/CRC},
    year = {2014},
    note = {ISBN 978-1466561595},
  }

@Manual{purrr_pkg,
    title = {purrr: Functional Programming Tools},
    author = {Hadley Wickham and Lionel Henry},
    year = {2025},
    note = {R package version 1.2.0},
    url = {https://CRAN.R-project.org/package=purrr},
  }

@article{borg_sharing_practices,
	title = {Comment on: {\textquoteleft}Moving Sport and Exercise Science Forward: A Call for the Adoption of More Transparent Research Practices{\textquoteright}},
	author = {Borg, David N. and Bon, Joshua J. and Sainani, Kristin L. and Baguley, Brenton J. and Tierney, Nicholas J. and Drovandi, Christopher},
	year = {2020},
	month = {08},
	date = {2020-08-01},
	journal = {Sports Medicine},
	pages = {1551--1553},
	volume = {50},
	number = {8},
	doi = {10.1007/s40279-020-01298-5},
	url = {https://doi.org/10.1007/s40279-020-01298-5},
	note = {Citation Key: borg{\_}sharing{\_}practices}
}

@article{ditroilo,
	title = {Exploratory Research in Sport and Exercise Science},
	author = {Ditroilo, Massimiliano and Mesquida, Cristian and Abt, Grant and Lakens, {Daniël}},
	doi = {10.51224/SRXIV.457},
	langid = {en}
}

@article{sainani_collaboration,
	title = {Call to increase statistical collaboration in sports science, sport and exercise medicine and sports physiotherapy},
	author = {Sainani, Kristin L. and Borg, David N. and Caldwell, Aaron R. and Butson, Michael L. and Tenan, Matthew S. and Vickers, Andrew J. and Vigotsky, Andrew D. and Warmenhoven, John and Nguyen, Robert and Lohse, Keith R. and Knight, Emma J. and Bargary, Norma},
	year = {2021},
	month = {01},
	date = {2021-01-01},
	journal = {British Journal of Sports Medicine},
	pages = {118--122},
	volume = {55},
	number = {2},
	doi = {10.1136/bjsports-2020-102607},
	url = {https://bjsm.bmj.com/content/55/2/118},
	note = {Citation Key: sainani{\_}collaboration},
	langid = {en}
}

@article{rasmussen2025,
	title = {Advancing physiology through transparency: Celebrating our first registered report},
	author = {Rasmussen, Peter and Tipton, Michael J. and Stewart, Alex and Bailey, Damian M.},
	year = {2025},
	date = {2025},
	journal = {Experimental Physiology},
	pages = {351--354},
	volume = {110},
	number = {3},
	doi = {10.1113/EP091963},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1113/EP091963},
	note = {{\_}eprint: https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/EP091963},
	langid = {en}
}

@article{impellizzeri2025,
	title = {Introducing a new {\textquotedblleft}Preliminary Report{\textquotedblright} submission category for small-sample intervention studies: rationale and instructions},
	author = {Impellizzeri, Franco M. and Murphy, Jennifer and Mesquida, Cristian and Warne, Joe and Hecksteden, Anne and Batomen, Brice and Wang, Chinchin and Meyer, Tim and Lakens, {Daniël}},
	year = {2025},
	month = {11},
	date = {2025-11-04},
	journal = {Science and Medicine in Football},
	pages = {1--11},
	volume = {0},
	number = {0},
	doi = {10.1080/24733938.2025.2580319},
	url = {https://doi.org/10.1080/24733938.2025.2580319},
	note = {Publisher: Routledge
{\_}eprint: https://doi.org/10.1080/24733938.2025.2580319
PMID: 41186160}
}

@Manual{irr,
  title = {irr: Various Coefficients of Interrater Reliability and Agreement},
  author = {Matthias Gamer and Jim Lemon and Ian Fellows Puspendra Singh <puspendra.pusp22@gmail.com>},
  year = {2019},
  note = {R package version 0.84.1},
  url = {https://CRAN.R-project.org/package=irr},
}

@Manual{here,
  title = {here: A Simpler Way to Find Your Files},
  author = {Kirill Müller},
  year = {2020},
  note = {R package version 1.0.1},
  url = {https://CRAN.R-project.org/package=here},
}