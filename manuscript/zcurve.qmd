---
title: "On the replicability of sports and exercise science research:"
subtitle: "Assessing the prevalence of selection bias and studies with underpowered designs by a z-curve analysis"
format: docx
bibliography: references.bib
csl: sports-medicine.csl
---

Cristian Mesquida¹², Jennifer Murphy², Joe Warne², and Daniël Lakens¹

#### Affilitation

¹ Human-Technology Interaction Group, Eindhoven University of Technology, Eindhoven, The Netherlands

² School of Biological, Health and Sports Sciences, Technological University Dublin, Tallaght, Dublin, Ireland

#### ORCIDs

Cristian Mesquida – 0000-0002-1542-8355

Jennifer Murphy – 0000-0001-8624-3828

Joe Warne – 0000-0002-4359-8132

Daniël Lakens – 0000-0002-0247-239X

#### Correspondence

Cristian Mesquida; Human-Technology Interaction Group, Eindhoven University of Technology, The Netherlands, c.mesquida.caldentey\@tue.nl

\pagebreak

```{r}
#| echo: false
#| message: false
#| warning: false

# Load packages
library(readxl)
library(dplyr)
library(here)
library(zcurve)
library(ggplot2)
library(knitr)
```

```{r}
#| echo: false
# Load data
zcurve_data <- read_xlsx("../data/zcurve_data.xlsx", sheet = "Sheet1")
```

```{r zcurve_analysis}
#| echo: false

# select p-values to be included in the z-curve analysis 
pvalues <- zcurve_data %>% 
  filter(include == "yes" & !is.na(zcurve_pvalue)) %>% 
  select(zcurve_pvalue) %>% 
  pull() %>% 
  as.numeric()

# primary zcurve analysis
primary_zcurve <- zcurve(p = pvalues)

# Obtain estimates and 95% CI
results <- summary(primary_zcurve, all = TRUE)
```

```{r excluded}
#| echo: false
excluded <- zcurve_data %>% 
  filter(include == "no") %>% 
  summarise(excluded = n()) %>% 
  mutate(proportion = round(excluded / nrow(zcurve_data), 2)*100)
```

```{r excluded_category}
#| echo: false
excluded_category <- zcurve_data %>% 
  mutate(reason_exclusion = case_when(
    reason_exclusion == "p <" ~ "not reported",
    reason_exclusion == "p >" ~ "not reported",
    TRUE ~ reason_exclusion)) %>%
  filter(include == "no") %>% 
  group_by(reason_exclusion) %>%
  summarise(number = n()) %>%
  mutate(proportion = round(number / sum(number) * 100, 0)) 
```

```{r}
#| echo: false
# Select studies that reported p as "p < 0.05" and were not included in the z-curve analysis
s_imprecise_pvalues <- zcurve_data %>%
  filter(include == "no" & pvalue == "p < 0.05") %>%
  select(include, pvalue, zcurve_pvalue)

# Convert "p < 0.05" to 0.25
s_imprecise_pvalues <- s_imprecise_pvalues %>%
  mutate(zcurve_pvalue = case_when(include == "no" & pvalue == "p < 0.05" ~ 0.25,
                             TRUE ~ as.numeric(zcurve_pvalue))) 
```

```{r}
#| echo: false 

# Select studies that reported p as "p > 0.05" and were not included in the z-curve analysis
ns_imprecise_pvalues <- zcurve_data %>%
  filter(include == "no" & pvalue == "p > 0.05") %>%
  select(include, pvalue, zcurve_pvalue)

# Convert "p > 0.05" to 0.5
ns_imprecise_pvalues <- ns_imprecise_pvalues %>%
  mutate(zcurve_pvalue = case_when(include == "no" & pvalue == "p > 0.05" ~ 0.05,
                             TRUE ~ as.numeric(zcurve_pvalue))) 
```

```{r}
#| echo: false 

included_as_exact <- zcurve_data %>% 
  filter(include == "yes") %>% 
  group_by(reason_exclusion) %>% 
  summarise(number = n(), 
            proportion = round(number / nrow(zcurve_data) * 100, 1))
```

```{r}
#| echo: false 

# Compute the required z-score to obtain 80% power
z_score <- round(power_to_z(0.8, alpha = 0.05), 1)

# Calculate the number of z-scores larger than 2.8
n_high_power <- sum(primary_zcurve$data > z_score) 

# Calculate the proportion of z-scores larger than 2.8
high_power <- round(sum(primary_zcurve$data > 2.801582) / length(primary_zcurve$data)*100, 0)
```

# Abstract

The Sports Science Replication Project has raised concerns about the replicability of published research. Low replication rates may result from an excess of significant findings caused by selection bias, where studies reporting significant findings are more likely to be published, inflating the proportion of significant findings in the literature, while the statistical power remains low. To date, no study has systematically assessed both selection bias and average statistical power in the same set of studies in the field. One method to assess selection bias and average statistical power is the $z$-curve method. In this study, we manually extracted `r nrow(zcurve_data)` independent *p*-values corresponding to the hypothesis tested in `r nrow(zcurve_data)` studies published across 10 applied sports and exercise science journals. After exclusions, a $z$-curve analysis was performed on `r nrow(zcurve_data) - excluded$excluded[1]` independent *p*-values. The estimate of the Observed Discovery Rate (`r round(primary_zcurve$N_sig/primary_zcurve$N_obs, 2)*100`%) is larger than the upper bound of the 95% confidence intervals (CI) of the Expected Discovery Rate of \[`r round(results$coefficients[2, 2], 2)*100`; `r round(results$coefficients[2, 3], 2)*100`%\] indicating strong publication bias in the literature. The average statistical power is `r round(results$coefficients[2, 1], 2)*100`% 95% CI \[`r round(results$coefficients[2, 2], 2)*100`; `r round(results$coefficients[2, 3], 2)*100`%\], and only `r high_power`% of studies are estimated to have been designed with high power ($\geq$ 80%). The Expected Replication Rate was `r round(results$coefficients[1, 1], 2)*100`% 95% CI \[`r round(results$coefficients[1, 2], 2)*100`; `r round(results$coefficients[1, 3], 2)*100`%\], indicating that only `r round(results$coefficients[1, 1], 2)*100`% of direct replications with the same sample size should be expected to replicate. Selection bias, combined with low average statistical power, is likely to result in a body of literature characterized by inflated effect sizes, a high proportion of type I and type II errors, and therefore low replicability. Addressing these issues requires a collective effort to build a more informative and reliable knowledge base.

#### Key points

The Observed Discovery Rate (68%) far exceeds the Expected Discovery Rate 95% \[`r  round(results$coefficients[2, 2], 2)*100`; `r round(results$coefficients[2, 3], 2)*100`%\], indicating significant selection bias in sports and exercise science research.

The average statistical power across studies is only `r round(results$coefficients[2, 1], 2)*100`% 95% CI \[`r round(results$coefficients[2, 2], 2)*100`; `r round(results$coefficients[2, 3], 2)*100`%\], with just `r high_power`% of studies designed with high power, suggesting many findings may be unreliable.

The average power of studies supporting the hypothesis is `r round(results$coefficients[1, 1], 2)*100`% implying that fewer than half of published significant findings are likely to replicate under the same conditions and sample size, undermining the replicability of the published literature.

\pagebreak

## Introduction

To appropriately evaluate how well scientific claims are empirically supported, it is essential that all observed results are reported in the published scientific literature. Problematically, there are clear signs of selective reporting, and statistically significant results are much more likely to be published than non-significant results [@scheel2022; @sterling1995]. Meta-scientific research has observed that between 73% and 81% of published studies in sports and exercise science journals report a statistically significant effect that supports the hypothesis that the researchers set out to test [@buttner2020; @mesquida_2023; @twomey_2021]. Is this estimate too high, and a possible sign of bias in the literature, or is it in line with what should be expected in an unbiased literature? The answer to this question depends on two unknown properties: the statistical power of the studies (henceforth, power), and the proportion of hypotheses that test true effects in the published literature [@scheel2022; @brunner_2020]. Power is the probability of observing a statistically significant effect if there is a true effect, and in turn depends on the sample size, the effect size, the statistical test that is performed, and the alpha level ($\alpha$). Although the power of studies is unknown, the average power of studies can be meta-analytically estimated under specific assumptions. Although the proportion of tested hypotheses that examine true effects is also unknown, we can examine how high the proportion of hypotheses testing true effects would need to be to achieve the observed rate of significant effects in the literature, given an estimate of the average statistical power. By comparing the proportion of studies supporting their tested hypotheses with the average power of those studies, we offer insights about the plausibility that selection bias is one of the contributing factors of low replicability in sports and exercise science.

The percentage of significant findings in a set of studies, also referred to as the Observed Discovery Rate (ODR), can be computed as follows:

$$
ODR = α × (1 − t) + (1 − β) × t 
$$ {#eq-1}

where $\alpha$ is the alpha level, *t* is the proportion of true hypotheses, $\beta$ is the type II error and 1 − $\beta$ is the power of a test [@scheel2022]. The Observed Discovery Rate in sports and exercise science of 73% to 81% can therefore result from a range of combinations of the power of tests and proportions of true hypotheses. For example, an Observed Discovery Rate of 73% can be achieved when 100% of the hypotheses tested are true, and the average power of studies is 73%, or when 73% of the hypotheses tested are true, and the average power of studies is close to 100%, or any combination in between these two extremes. @fig-fig1.1 visualizes the relationship between power and the proportion of true hypotheses, and as illustrated, the lower the proportion of true hypotheses is, the higher the power of the tests must be.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-fig1.1
#| fig-width: 5
#| fig-height: 3
#| fig-cap: "Combinations of the proportion of true hypotheses (x-axis) and power (y-axis) required to produce 73% or 81% statistically significant findings assuming an alpha level of $\\alpha = 0.05$ and no bias. Notably, achieving 73% significant findings requires the average power to be at least 73% if all tested hypotheses are true."

# The formula for ODR is as follows: ODR = a*(1-t) + (1-B)*t, 
# where (1-B) = statistical power, a = alpha level, t = proportion of true hypothesis
power <- 100
a <- 0.05 
t <- 0.73

# If we aim to solve for power, the formula can be rearranged as follows: 
# 1-B = (ODR - a + a*t)/t
ODR <- 0.73 
a <- 0.05 
t <- 1

# Set parameters
ODR <- c(0.73, 0.81) # observed discovery rate
a <- 0.05 # alpha level
t <- 1:1000 / 1000  # proportion of true hypotheses from 0.01 to 1.00

# Create a data frame to store the results
results <- expand.grid(ODR = ODR, t = t)
results$power <- (results$ODR -a + a*results$t)/results$t
results$ODR <- as.factor(results$ODR)
results <- results %>% 
  filter(power < 1.5)

# Create Figure 1
ggplot(data = results) +
  geom_line(aes(y = power, x = t, linetype = ODR), size = 1) +
  scale_x_continuous(name="proportion of true hypotheses",
                     limits=c(0.6, 1),
                     breaks = c(seq(0.6, 1, 0.1)),
                     expand = c(0, 0)) +
  scale_y_continuous(name="average statistical power",
                     limits=c(0.6, 1),
                     breaks = c(seq(0.6, 1, 0.1)),
                     expand = c(0, 0)) +
  theme_bw()
```

The assumption that sports and exercise scientists almost exclusively examine true hypotheses seems unrealistic, and this assumption is not in line with empirical evidence from other fields [@szucs2017; @wilson2018], or with the results of the sports science replication project [@murphy_replicability_2025]. A field that only studies true hypotheses is arguably not sufficiently pushing the boundaries of our current understanding. Less is known about how reasonable the assumption is that studies in sports and exercise science achieve an average power of 73%. If this is not the case, then the high percentage of significant findings could only be explained by bias towards significant effects in the published literature. Given the small sample sizes reported in the field [@mesquida_2023; @Abt2020], it seems doubtful that the average power could be as high as 73%. However, high power could be achieved if studies investigate large effects, use within-subject designs or repeated measures, or include appropriate covariates to account for a greater proportion of the error variance. The aim of the current study is to provide the first systematic assessment of the average power of the published sports and exercise science literature to inform future discussions about the presence of selection bias in the field, and its potential effect on the replicability of sports and exercise science.

It is important to distinguish between two broad categories of bias: “publication bias” [@mahoney1977; @rosenthal1979] and “*p*-hacking” [@stefan2023]. Publication bias occurs when the studies in the scientific literature are systematically unrepresentative of the studies that are performed. It is often caused by the tendency of editors, reviewers, and researchers to prefer studies that support the hypothesis tested over those that fail to support it (e.g., significance bias). In the context of NHST, a study reporting a significant *p*-value would provide support in favor of the hypothesis tested, and thus would be more likely to get published than a study that failed to support the same hypothesis. *P*-hacking can be defined as a set of problematic practices that opportunistically exploit flexibility in data collection and analysis to render non-significant findings significant. Surveys among scientists suggest that *p*-hacking is widespread across disciplines [@lakens_2024], and therefore, there is no reason to believe that sport and exercise science is an exception. In a research environment shaped by publication pressures and incentives that reward significant findings, researchers might resort to *p*-hacking to render their non-significant effect significant and thus increase their chances of publication. A main consequence of publication bias and *p*-hacking is that the percentage of significant findings in the literature is higher than the average power of studies, contributing to an excess of significant findings that reflect type I errors. An excess of significant findings has been reported in sports therapy and rehabilitation [@borg_2023] and in the *Journal of Sports Sciences* [@mesquida_2023]. Whether these findings can be generalized to the field of sport and exercise science has yet to be established and is the primary focus of the current study.

**Z-curve**

One method that models the presence of selection bias and identifies underpowered designs from the distribution of *p*-values is the $z$-curve method [@bartos_2022]. Briefly, the $z$-curve method transforms reported *p*-values into absolute $z$-scores and compares the observed and expected distribution of $z$-scores. If these two distributions are sufficiently similar, there is no indication of bias, whereas large differences between the observed and expected distribution suggest the presence of bias. The $z$-curve method estimates 4 quantities that provide insights into the replicability of a literature, under specific assumptions [@brunner_2020]. First, $z$-curve assumes that observed $z$-scores are obtained from multiple sampling distributions with different means, allowing for heterogeneity in power estimates. This makes $z$-curve a better choice for our study as opposed to the related *p*-curve analysis, since high heterogeneity can be expected when studies are selected from different sub-disciplines such as sports performance, exercise physiology, biomechanics, and sports psychology.

Second, $z$-curve assumes that all *p*-values are independent. This assumption is met if, as in our study, only one *p*-value per study is included in the $z$-curve analysis. Finally, $z$-curve assumes that all studies used the same criterion for statistical significance ($\alpha$ = 0.05). Thus, if a study corrected for multiple comparisons or used a more conservative criterion (e.g., $\alpha$ = 0.01), bias is modeled based on an $\alpha$ of 0.05 instead of the actual, more conservative criterion. This is a conservative assumption, as $z$-curve will overestimate replicability when the assumed $\alpha$ is higher than the actual $\alpha$. The 4 quantities computed in a $z$-curve analysis are described in @tbl-tbl1.1.

```{r}
#| echo: false

alpha <- 0.05
true_effect <- 0.7
beta <- 0.2

odr <- round(alpha * (1 - true_effect) + (1 - beta) * true_effect, 2)*100
```

```{r}
#| echo: false

df <- data.frame(
  study = c(1, 2, 3),
  power = c(10, 50, 90)
  )
```

```{r}
#| echo: false

alpha <- 0.05
edr <- (df$power[1] + df$power[2] + df$power[3]) / nrow(df)

mfdr <- round(((1 - (edr/100)) / (edr/100)) * (alpha / (1 - alpha)), 3)
```

| Quantity | Description |
|:-------------------------|:---------------------------------------------|
| Observed Discovery Rate (ODR) | The ODR is the rate of studies reporting a significant *p*-value that would support the hypothesis tested. However, this rate is not necessarily an accurate reflection of true effects because this rate also includes type I errors. For instance, imagine that researchers test 100 hypotheses of which `r true_effect*100` correspond to true effects and 30 correspond to null effects. Furthermore, assume that researchers design their studies with 80% power and set $\alpha$ to 0.05. Using @eq-1, the ODR would be `r odr`% (`r alpha` × (1 – `r true_effect`) + (1 - `r beta`) × `r true_effect` = `r odr`). Out of these `r odr` significant effects, `r (true_effect*100) * (1 - beta)` would correspond to true effects (`r true_effect*100` x `r 1 - beta`) and \~ `r round(((1 - true_effect)*100)*alpha, 0)` would correspond to type I errors (`r (1 - true_effect)*100` x `r alpha`). Furthermore, publication bias inflates the ODR. Following with the previous example, if `r ((1 - true_effect)*100)/2` out of the `r (1 - true_effect)*100` null effects were not published due to publication bias, the ODR would be `r round(((((true_effect*100) * (1 - beta)) + round(((1 - true_effect)*100)*alpha, 0)) / (100 - ((1 - true_effect)*100)/2)*100), 0)`% ((`r (true_effect*100) * (1 - beta)` + `r round(((1 - true_effect)*100)*alpha, 0)`) / `r 100 - ((1 - true_effect)*100)/2` = `r round(((((true_effect*100) * (1 - beta)) + round(((1 - true_effect)*100)*alpha, 0)) / (100 - ((1 - true_effect)*100)/2)*100), 0)`). Publication bias and *p*-hacking increase the proportion of type I errors, thereby inflating the ODR. |
| Expected Discovery Rate (EDR) | The EDR is an estimate of the average power of all studies included in the $z$-curve analysis. For example, if we have 3 studies designed with `r df$power[1]`%, `r df$power[2]`% and `r df$power[3]`% power, the EDR would be `r (df$power[1] + df$power[2] + df$power[3]) / nrow(df)`% ((`r df$power[1]` + `r df$power[2]` + `r df$power[3]`)/`r nrow(df)`). The EDR can be compared to the ODR to determine the presence of selection bias. If the point estimate of the ODR is larger than the upper bound of the 95% CI of the EDR, we can statistically reject the hypothesis that there is no selection bias [@bartos_2022]. |
| Expected Replication Rate (ERR) | The ERR is an estimate of the average power of studies reporting a significant *p*-value. This estimate reflects the probability of observing the same significant effect if the study were to be replicated using the same sample size and following the same procedures. Using the prior example, if the study with a 90% power was the only one that yielded a significant finding, then the ERR would be 90%. Thus, the higher the ERR, the more likely it is that the studies that reported a significant effect would replicate. |
| Maximum False Discovery Risk (MFDR) | The MFDR is an estimate of the maximum percentage of significant findings that are type I errors. Importantly, the MFDR does not aim to estimate the actual type I error rate among significant $p$-values. Rather, it provides an estimate of the worst-case scenario with the highest possible proportion of type I errors. If a literature has a low MFDR, readers can be assured that most significant findings are true effects. The MFDR is estimated using the Expected Discovery Rate and $\alpha$, and it is computed as: MFDR = ((1 - $EDR$) / $EDR$) $\times$ ($\alpha$ / (1 - $\alpha$)). For example, with an EDR of `r (df$power[1] + df$power[2] + df$power[3]) / nrow(df)`% and $\alpha$ = 0.05, the MFDR is `r mfdr`, which is close to the nominal $\alpha$ set to 0.05. |

: Description of the 4 quantities estimated by $z$-curve. {#tbl-tbl1.1}

**Simulations**

The distribution of *p*-values (and therefore the distribution of $z$-scores) in the published literature is determined by four factors, namely, the proportion of studies that investigate true and null effects, the power of studies that investigate true effects, publication bias[,]{.underline} and *p*-hacking. To help readers understand which findings to expect from a $z$-curve analysis, we first simulate 300 *p*-values to represent six distinct scenarios, and then conduct the corresponding $z$-curve analysis. The rationale behind these scenarios is to provide a diverse set of conditions, illustrating how the distribution of $z$-scores is affected by power and selection bias. We simulated 300 *p*-values because this closely represents the number of *p*-values reported in our study (i.e., $n$ of 269 after exclusions). For all six scenarios, the *p*-values were generated using an unpaired one-tailed *t*-test and $\alpha$ of 0.05. The code for the simulations and the $z$-curve analyses is available at [https://osf.io/d7wyc/](https://osf.io/d7wyc/.%22).

In the first scenario (@fig-fig1.2-1; “80% power”), *p*-values are simulated based on a true effect size (Cohen’s *d~s~*) of 0.3 and a total sample size ($n$) of 278, which yields a power of \~ 80%. In the second scenario (@fig-fig1.2-2; “Null effect”), *p*-values are simulated based on a true effect size of 0, and therefore the number of significant findings corresponds to $\alpha$. That is, \~ 5% of studies report a significant effect, but all these findings are type I errors, as there is no real effect to be found. In the third scenario (@fig-fig1.2-3; “Publication bias and 20% power”), *p*-values are simulated based on a true effect size of 0.3 and $n$ of 30, which yields a power of \~ 20%. Additionally, publication bias is introduced, such that 40% of the non-significant findings remain unpublished. In the fourth scenario (@fig-fig1.2-4; “Publication bias and null effect”), *p*-values are simulated based on a true effect size of 0 and $n$ = 30. Additionally, publication bias is introduced, such that 90% of the non-significant findings remain unpublished. In the fifth scenario (@fig-fig1.2-5; “Mild optional stopping”), *p*-values are simulated based on a true effect size of 0 and a mild optional stopping strategy where researchers perform a maximum of 5 hypothesis tests. When researchers engage in optional stopping, they repeatedly perform a hypothesis test after adding new participants, until either the maximum sample size that researchers are willing to recruit is achieved (in the scenario $n$ of 50), or a significant *p*-value is observed, without correcting $\alpha$ for multiple comparisons. In the last scenario (@fig-fig1.2-6; “Mixed”), the *p*-values are simulated as follows: 300 *p*-values are first simulated, of which 100 are based on a true effect size of 0.3 and $n$ of 278, 100 are based on a true effect size of 0.3 and $n$ of 100, and 100 are based on a true effect size of 0.3 and $n$ of 26. 100 of the non-significant *p-*values have been then randomly replaced by *p*-values obtained through severe optional stopping. The distributions of $z$-scores under each scenario are presented in @fig-fig1.2, and the corresponding results of each $z$-curve are presented in @tbl-tbl1.2.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-fig1.2
#| fig-subcap: true
#| fig-subrefs: true
#| layout-ncol: 2
#| fig-align: center
#| fig-width: 4
#| fig-height: 4
#| out.width: "70%"
#| fig-cap: "Distribution of 300 $z$-scores over the interval 0-6 across six scenarios: (a) ~80% power; (b) no true effect; (c) publication bias and ~20% power; (d) no true effect with publication bias; (e) mild optional stopping with no true effect; (f) mixture of studies designed with high and low power with publication bias and optional stopping. The red line marks the $z$-score of 1.96, the critical value for statistical significance ($\\alpha = 0.05$). The blue line shows the expected density distribution of $z$-scores, with dotted lines representing its 95% CI."

include_graphics(
  c(
  "../figures/individual_figures2/figure2a.png",
  "../figures/individual_figures2/figure2b.png",
  "../figures/individual_figures2/figure2c.png",
  "../figures/individual_figures2/figure2d.png",
  "../figures/individual_figures2/figure2e.png",
  "../figures/individual_figures2/figure2f.png"
  )
)
```

```{r}
#| echo: false
#| message: false
#| label: tbl-tbl1.2
#| tbl-cap: "Output estimates [95% CI] for the $z$-curves conducted under six scenarios."

table2 <- readRDS(here("data", "table2.rds"))

kable(
  table2,
  align = "l",
  booktabs = TRUE
)
```

1 = 80% power; 2 = Null effect, 3 = Publication bias and 20% power; 4 = Publication bias and null effect; 5 = Mild optional stopping; 6 = Mixed

The results of these simulations illustrate several key points. In the absence of bias, when studies are designed with 80% power, the average power of studies matches the Observed Discovery Rate, whose estimate lies within the 95% CI of the Expected Discovery Rate. The Maximum False Discovery Risk is close to $\alpha$ because studies are designed with high power. When studies investigate a null effect, the Observed Discovery Rate corresponds to $\alpha$, and the Maximum False Discovery Risk is 1, as all significant findings are type I errors. In the presence of publication bias and studies with 20% power, the Observed Discovery Rate becomes inflated and exceeds the upper limit of the Expected Discovery Rate 95% CI. This discrepancy indicates bias in the set of studies. Furthermore, when the Expected Discovery Rate does not exclude 5%, it suggests that all observed effects may be type I errors. Under a true effect size of zero combined with publication bias, the Observed Discovery Rate becomes substantially larger than $\alpha$. In the fifth scenario, where optional stopping was simulated under a true effect size of zero, the Observed Discovery is again inflated beyond $\alpha$. The fact that the Observed Discovery Rate is higher than the upper bound of the Expected Discovery Rate 95% CI further signals bias. It is important to note, however, that although the $z$-curve method can be used to assess publication bias, it is not developed to identify *p*-hacking, and the $z$-curve method might not be able to distinguish between publication bias and *p*-hacking. In this scenario, the Expected Discovery Rate is 5% which corresponds to the expected type I error under the null distribution. When the Expected Discovery Rate does not exclude 5%, it suggests that all effects might be, in fact, null effects. Additionally, the Maximum False Discovery Risk is 1, in line with the fact that all significant findings are type I errors. Finally, in the mixed scenario, the Observed Discovery Rate exceeds the upper bound of the Expected Discovery Rate 95% CI, indicating the presence of bias, and highlighting the limitation that $z$-curve analysis cannot differentiate publication bias from *p*-hacking.

To sum up, the $z$-curve method can be used to distinguish between an unbiased and biased published literature by comparing the Observed Discovery Rate, the Expected Discovery Rate and the Expected Replication Rate. In the absence of bias and high average power, the Observed Discovery Rate should lie inside the 95% CI of the Expected Discovery Rate and the higher the power, the more $z$-scores should be larger than 3 (i.e., *p* \< 0.001). In such case, the published literature is characterized by studies investigating true effects with high-power designs and therefore it should be expected to be highly replicable in direct replications. On the contrary, if the Observed Discovery Rate is larger than the upper bound of the 95% CI of the Expected Discovery Rate, the published literature is biased and researchers have reasons to doubt the likelihood that effects will replicate. Put even more simply, the blue solid line shows the expected distribution of *p*-values in all simulations. In the cases where there is a questionable absence of observed *p*-values below this line, in particular to the left of the red line of z = 1.96 (representing non-significant effects), the model would indicate evidence of bias.

To date, there is no study that has complemented the percentage of significant studies with the average power in the same sample of studies in sports and exercise science, which is required to interpret whether there is an excess of significant findings. Furthermore, although the recently reported replication rate of the sports science replication project [@murphy_replicability_2025] should provide empirical evidence of the low replicability of the field, skeptical sport and exercise scientists may argue that such low replication rates are not representative of the field due to the small number of replications conducted. Alternatively, they might attribute failures to replicate original studies to deviations from the original studies, replication studies with underpowered designs, unaccounted experimental factors, or even to a bias towards non-replication by replication labs. The study aims to contribute to the sports science replication project by providing empirical support for the idea that low replication rates are at least in part caused by selection bias. Specifically, we assess the presence of selection bias and average power in a sample of 269 studies published across ten applied sports and exercise science journals using a $z$-curve analysis of primary statistical results.

## Methods

This is a retrospective observational study. The preregistration of this study can be found at <https://osf.io/d7wyc/>.

### Study sample size

A sample of 350 studies was used for the purpose of this study. As stated in the preregistration, this sample size was based on a precision analysis conducted for a previous study to estimate an expected proportion of 30% of studies reporting an a priori power analysis (<https://osf.io/mqbr2/>). A precision analysis can estimate the number of observations required to determine an expected proportion within a specified margin of error. To estimate a proportion of 30% with a margin of error of 5%, the precision analysis returned a sample of 323 studies, which was rounded up to 350 studies. The same set of 350 studies was used in both the previous study and the present study.

### Journal and study selection protocol

The 350 studies were sampled from 10 journals ranked in quartile 1 according to www.scimagojr.com (as of 13th September 2022). The list of journals, along with the number of studies sampled from each, is depicted in @fig-fig1.3. All studies were published between 2024 and 2018. We started at a given issue and worked backwards. The study selection protocol was based on the Proposal of a Selection Protocol for Replication of Studies in Sports and Exercise Science [@murphy_2022]. First, only applied sport and exercise science studies (studying changes in human performance in response to physical activity, exercise, and sport) in the sub-disciplines of physiology, sports performance, physical activity, injury prevention, and psychology were considered. Second, only confirmatory studies that tested a hypothesis with an experimental (randomized controlled trials) or quasi-experimental design (non-randomized controlled trials) were included. Third, studies had to use an $F$-test (i.e., ANOVA) or $t$-test as an inferential test to evaluate the hypothesis; studies that employed correlations, mixed models or Bayesian statistics were excluded. We followed the protocol described in Murphy et al. [-@murphy_2022] to select studies, with two deviations from the original protocol. First, whereas the original protocol only selected studies that reported a statistically significant main effect, we considered both statistically significant and non-significant effects. This is because the $z$-curve analysis uses both significant and non-significant $p$-values. Second, we also considered interaction effects, which were not considered in the original selection protocol.

```{r}
#| echo: false
#| label: fig-fig1.3
#| fig-width: 10
#| fig-height: 5
#| fig-cap: "List of journals from which the 350 independent p-values were extracted, along with the number of studies sampled from each journal and the corresponding proportion."

journal_summary <- zcurve_data %>%
  rename(Journal = journal) %>%
  count(Journal, name = "Number") %>%
  mutate(
    Proportion = Number / nrow(zcurve_data) * 100,
    Label = paste0(round(Proportion, 0), "%")  # proportion as label
  ) %>%
  arrange(Number) %>%  # order from smallest → largest
  mutate(Journal = factor(Journal, levels = Journal))

# Create the horizontal bar plot
ggplot(journal_summary, aes(x = Number, y = Journal)) +
  geom_col(fill = "grey70") +
  geom_text(
    aes(label = Label),
    hjust = -0.1,   # label just outside the bar
    size = 4
  ) +
  labs(
    x = "Number of studies",
    y = NULL
  ) +
   scale_x_continuous(
    limits = c(0, 50),        # set x-axis from 0 to 50
    expand = c(0, 0)          # remove extra padding
  ) +
  coord_cartesian(clip = "off") +
  theme_classic() +
  theme(
    axis.text.y = element_text(size = 10),
    plot.margin = margin(5.5, 60, 5.5, 5.5)
  )
```

### Inter-rater reliability

```{r intercoder_agreement}
#| echo: false
#| message: false

# Load script to calculate intercoder agreement
source(here("r_scripts", "intercoder_agreement", "03_intercoder_agreement.R"))
```

```{r average_fleiss}
#| echo: false

# Calculate average Fleiss' kappa for "support_hypothesis", "type_hypothesis" and
# "statistical_result", "type_effect"
average_fleiss <- round(mean(agreement_3_coders[c(16, 17, 19, 20), 2]), 2)
```

```{r average_cohen}
#| echo: false

# Calculate average Cohen's Kappa for "support_hypothesis", "type_hypothesis",
# "statistical_result" and "type_effect"
average_cohen <- round(mean(agreement_2_coders[c(16, 17, 19, 20), 2]), 2)
```

Prior to collecting any data, and in anticipation of difficulties to select the statistical result central to the tested hypothesis, we developed a coding strategy over a four-step process. First, the four authors (CM, JM, DL and JW) developed and discussed the coding form created by CM. Ambiguities in the coding form were discussed, and amendments were made. Second, three raters (CM, JM and JW) independently coded a randomly selected subset of 28 studies from the sample pool of studies (350) as a pilot study. Subsequently, raters’ responses were compared, and any disagreements were used to improve the clarity of the coding form. Third, the same three raters (CM, JM and JW) independently coded a second random subset of 19 studies. Inter-rater agreement was assessed by calculating a pooled Fleiss’ Kappa estimate for each coding category across the 47 studies coded in the first two rounds of coding. The mean inter-rater agreement for the categorical responses was `r average_fleiss`, indicating substantial agreement. The second round of coding was also used to discuss disagreements. The remaining 303 studies were double-coded whereby both JM and JW each coded \~176 studies and CM coded the full sample of studies (350). Inter-rater agreement was assessed by calculating a pooled Cohen’s Kappa estimate. The inter-rater agreement for the final round of coding was `r average_cohen`, indicating almost perfect agreement. Inter-rater agreements across variables and the coding form can be found at <https://osf.io/d7wyc/>. After termination of data collection, any discrepancies in coding decisions were resolved through discussion between the two pairs of raters and can be found at <https://osf.io/d7wyc/>. DL provided guidance when discrepancies arose and agreement between two raters could not be reached.

### Procedures and data extraction

The $z$-curve analysis is an example of a *p*-value meta-analysis and is based on the manually coded *p*-values from the 350 studies. Only one *p*-value per study was extracted, which corresponded to the main statistical test for the central hypothesis of each study. Because hypotheses statements often include vague language and the primary dependent variable is not always operationalised clearly, we used a coding strategy that consisted of several steps to select the key dependent variable. First, the selected dependent variable would be the one for which researchers controlled for both type I and type II error rates. Specifically, in addition to controlling for type I error, researchers conducted an a priori power analysis to control for type II error. Thus, the key dependent variable should be listed in both the a priori power analysis and hypothesis statements. However, on some occasions, the dependent variable stated in the a priori power analysis would not match the dependent variable stated in the hypothesis. In these cases, we would select the dependent variable stated in the hypothesis if it is clearly identifiable. Often, the statistical result central to the hypothesis tested was difficult to identify due to the lack of a priori power analysis and the vagueness of the hypothesis tested. This included hypothesis statements that predicted the effect of one or several interventions on more than one dependent variable or a dependent variable that was measured in multiple ways. In those cases, we selected a dependent variable linked to the central hypothesis test and listed in: 1) the sentence describing the aim of the study; 2) the abstract; 3) the title; 4) or the results, in this order of priority. We selected the dependent variable that best matched the language the authors use to imply the focus of the study, in cases where there were still several dependent variables listed. For each study the following pieces of information were extracted the a priori power analysis statement, the hypothesis statement, whether the hypothesis predicted the presence or absence of an effect, the type of effect (i.e., a mean difference, a main effect or interaction effect), the statistical result including the degrees of freedom, the test statistic, the effect sizes and its CI, and the *p*-value. A disclosure table containing all extracted information used to justify the coding decisions regarding the selected key statistical result for each selected study can be found at <https://osf.io/d7wyc/>.

### Recomputing *p*-values

The $z$-curve method requires exact *p*-values (e.g., *p* = 0.002) as input. If the corresponding *p*-value was reported relatively (e.g., *p* \< 0.05), we attempted to recompute the *p*-value when sufficient information was available (i.e., degrees of freedom and *F*-ratio or *t*-statistic). *P*-values were recomputed in Microsoft Excel using the functions T.DIST.2T or F.DIST.RT for *t*-tests and *F*-tests, respectively. These functions require both the test statistic and degrees of freedom. In case where a *t*-statistic or *F*-ratio from a one-way ANOVA with two levels was reported but the degrees of freedom were not reported, the degrees of freedom were determined using the sample size per group and study design reported in the original study. When the exact *p*-value and the corresponding statistic were not reported, but an effect size was available, we attempted to convert effect sizes into *p*-values for study designs involving a *t*-test and one-way ANOVAs with two levels. Formulas used to recompute *p*-values from effect sizes can be found in the supplementary information at <https://osf.io/d7wyc/>. We did not attempt to compute other ANOVA effect sizes (i.e., $\omega^2$, $\omega_p^2$) because they require information that is seldom reported in articles, such as mean-square (MS) and sum-of-squares (SS) errors.

### Study exclusions

*P*-values reported as *p* \< 0.05 or *p* \> 0.05, which could not be recomputed in their exact form, were excluded. There is no optimal decision in how to deal with *p*-values in studies where results are underreported, and exact $p$-values cannot be recomputed, which stresses the importance of fully reporting the results of statistical tests. Second, *p*-values extracted from studies that tested the hypothesis of no effect or equivalence using a classic hypothesis test were not included. Similarly, $p$-values obtained from studies that tested a directional hypothesis but obtained a significant result in the other direction were not included because they can also distort the results of the $z$-curve. Finally, studies that used a mixed design but did not directly compare two interventions were excluded, such as claims that one intervention is superior to a control condition after observing a pre-post significant difference in the intervention group, while the corresponding pre-post difference in the control group is not significant. Performing two paired *t*-tests is statistically invalid because it does not test the hypothesis that researchers set out to test (i.e., one intervention is superior or inferior to the other), which would require a direct comparison between the two groups [@bland_2011].

Out of the `r nrow(zcurve_data)` independent *p*-values extracted, `r excluded$excluded[1]` (`r excluded$proportion[1]`%) were excluded. Among those, `r excluded_category$number[1]` (`r excluded_category$proportion[1]`%) could not be recomputed into an exact *p*-value, `r excluded_category$number[2]` (`r excluded_category$proportion[2]`%) studies tested the hypothesis of no difference without using an equivalence test, `r excluded_category$number[3]` (`r excluded_category$proportion[3]`%) studies reported a significant *p*-value in the opposite direction as predicted, for `r excluded_category$number[4]` (`r excluded_category$proportion[4]`%) studies the key statistical result was unclear, and `r excluded_category$number[5]` (`r excluded_category$proportion[5]`%) used a within-subject comparison instead of an interaction. Therefore, and a total of `r nrow(zcurve_data) - excluded$excluded[1]` *p*-values were converted into $z$-scores to fit the $z$-curve model.

### Study deviations

In the preregistration, it was stated that studies reporting absolute *p*-values (e.g., $p$ \> 0.05) that could not be recomputed into their exact form would not be included. However, $p$-values reported as $p$ \< 0.001 or $p$ \< 0.005 were coded as $p$ = 0.0001 and $p$ = 0.0005, respectively, and included in the $z$-curve analysis. This decision represented a deviation from the preregistration. We made this conservative decision because it is common (and defensible) to report results with such small *p*-values using the ‘smaller than’ notation, and this reporting strategy is more likely to be observed for studies investigating true effects with high power. Excluding such studies would bias our inclusion criteria towards lower-powered studies, while deviating from our preregistration leads to the inclusion of studies with higher power.

### Statistical analysis and software

All simulations and $z$-curve analyses were performed in R (`r version$version.string`; @Rcoreteam) using the *zcurve 2.0* package [@zcurve_pkg]. R packages used to produce this manuscript include *readxl* [@readxl_pkg], *dplyr* [@dplyr_pkg], *ggplot2* [@ggplot2_pkg], *knitr* [@knitr_pkg] and *purrr* [@purrr_pkg]. The manuscript was written in *Quarto* [@quarto_pkg]. Data and analysis scripts related to this study are publicly available on the Open Science Framework and can be found at <https://osf.io/d7wyc/>.

```{r zcurve}
#| echo: false

# select p-values to be included in the z-curve analysis 
pvalues <- zcurve_data %>% 
  filter(include == "yes" & !is.na(zcurve_pvalue)) %>% 
  select(zcurve_pvalue) %>% 
  pull() %>% 
  as.numeric()

# primary zcurve analysis
primary_zcurve <- zcurve(p = pvalues)

# Obtain estimates and 95% CI
results <- summary(primary_zcurve, all = TRUE)
```

## Results

Out of all `r nrow(zcurve_data) - excluded$excluded[1]` included *p*-values, `r included_as_exact$number[1] + included_as_exact$number[2] + included_as_exact$number[3]` were imputed as follows: `r included_as_exact$number[1]` were reported as *p* \< 0.001, `r included_as_exact$number[2]` as *p* \< 0.003 and `r included_as_exact$number[3]` (1/350) as *p* \< 0.005, which were coded as *p* = 0.0001, *p* = 0.0003 and *p* = 0.0005, respectively, and were included in the $z$-curve model. In addition to the sensitivity analysis excluding these $p$-values, further sensitivity analyses were conducted at the reviewer’s request. These included: (2) a sensitivity analysis in which `r nrow(s_imprecise_pvalues)` $p$-values reported as $p$ \< 0.05 that could not be recomputed and were not included in the primary $z$-curve were imputed as 0.25; (3) a sensitivity analysis in which `r nrow(ns_imprecise_pvalues)` $p$-values reported as $p$ \> 0.05 that could not be recomputed and were not included in the primary $z$-curve were imputed as 0.5; and (4) a sensitivity analysis combining both sets of imputed $p$-values. All four sensitivity analyses returned results consistent with those of the primary $z$-curve. The results of the sensitivity analyses can be found at <https://osf.io/d7wyc/>.

The results of the $z$-curve analysis are shown in @fig-fig1.4. The Observed Discovery Rate was `r round(primary_zcurve$N_sig/primary_zcurve$N_obs, 2)*100`% (95% CI \[62; 74\]), indicating that `r round(primary_zcurve$N_sig / primary_zcurve$N_obs, 2)*100`% of sampled studies supported the hypothesis tested. The Expected Discovery Rate was `r round(results$coefficients[2, 1], 2)*100`% (95% \[`r round(results$coefficients[2, 2], 2)*100`; `r round(results$coefficients[2, 3], 2)*100`\]), indicating an average power of `r round(results$coefficients[2, 1]*100, 0)`% for studies reporting both significant and non-significant results. The Expected Replication Rate was `r round(results$coefficients[1, 2], 2)*100`% (95% CI \[`r round(results$coefficients[1, 2], 2)*100`; `r round(results$coefficients[1, 3], 2)*100`\]) indicating that studies reporting significant results have an average power of `r round(results$coefficients[1, 1]*100, 0)`%. This suggests that if we were going to conduct direct replications with the sample size of the original studies reporting significant findings, only `r round(results$coefficients[1, 1]*100, 0)`% of these studies would be expected to yield another significant effect. Selection bias can be examined by comparing the Observed Discovery Rate (the percentage of significant results in the set of studies) to the Expected Discovery Rate (the proportion of the area under the curve on the right side of the significance criterion). The point estimate of the Observed Discovery Rate (`r round(primary_zcurve$N_sig/primary_zcurve$N_obs, 2)*100`%) is larger than the upper bound of the 95% CI of the Expected Discovery Rate (\[`r round(results$coefficients[2, 2], 2)*100`; `r round(results$coefficients[2, 3], 2)*100`\]), suggesting that we can statistically reject the null hypothesis of no selection bias. The point estimate of the Maximum False Discovery Risk was `r round(results$coefficients[3, 1],2)*100`% (95% CI \[`r round(results$coefficients[3, 2], 2)*100`; `r round(results$coefficients[3, 3], 2)*100`\]), indicating that, in a worst-case scenario, an estimated `r round(results$coefficients[3, 1], 2)*100`% of the significant effects could be type I errors. The point estimate of the File-Drawer Ratio was `r round(results$coefficients[4, 1], 0)` (95% CI \[`r round(results$coefficients[4, 2], 0)`; `r round(results$coefficients[4, 3], 0)`\]), suggesting that for every published significant result, $z$-curve predicts `r round(results$coefficients[4, 1], 0)` unpublished studies with non-significant results. Finally, a visual inspection of @fig-fig1.4 also indicates that `r n_high_power` out of `r (nrow(zcurve_data) - excluded$excluded[1])` (`r high_power`%) had $z$-scores greater than `r z_score`, which indicates the presence of studies investigating true effects with high-power designs ($\geq$ 80%).

```{r}
#| echo: false
#| label: fig-fig1.4
#| fig-width: 6
#| fig-height: 5
#| fig-cap: "Distribution of 269 $z$-scores over the interval 0-6. The vertical red line refers to a $z$-score of 1.96, the critical value for statistical significance ($\\alpha = 0.05$). The dark blue line is the density distribution for the inputted *p*-values (represented in the histogram as $z$-scores). The dotted lines represent the 95% CI for the density distribution. Range represents the minimum and maximum values of $z$-scores used to fit the $z$-curve."

plot(primary_zcurve, CI = TRUE, annotation = TRUE, main = "")
```

## Discussion

The first aim of this meta-study was to estimate the average power of studies published across ten journals by conducting a $z$-curve analysis. The Expected Discovery Rate—the average power of studies reporting a significant and non-significant effect—was only `r round(results$coefficients[2, 1], 2)*100`% (95% CI \[`r round(results$coefficients[2, 2], 2)*100`; `r round(results$coefficients[2, 3], 2)*100`\]), which is much lower than the minimum recommended level of power of 80%. Despite the low average power, `r n_high_power` out of the `r nrow(zcurve_data) - excluded$excluded[1]` (`high_power`%) studies included in the $z$-curve analysis had $z$-scores greater than `r z_score`, suggesting that some studies tested true effects with high-power designs ($\geq$ 80%). In other words, while average power is low, approximately one quarter of studies seem to have been designed with adequate power, likely due to examining large effects, using large sample sizes, or both. Conversely, many studies had extremely low power, in some cases approaching the lower limit of 5%—the type I error rate—, which is the expected probability of a significant result if the true effect size is zero. Low power is not unique to sports and exercise science but a recurrent issue across disciplines [@button2013; @maxwell2004; @quintana_2020]. Studies designed with low power yielding non-significant effects have low informational value because such findings have a high probability of being a type II error. Moreover, most studies in the field lack sufficient sample sizes to perform an equivalence test with adequate statistical power, preventing researchers from statistically assessing the absence of meaningful effects. This is because the narrower the equivalence bounds, or the smaller the effect sizes one tries to reject, the larger the required sample size [@lakens_equivalence_2017]. Studies with underpowered designs also increase the uncertainty around the true effect size, as reflected in the width of the CI. For instance, a study conducted with a small sample that reports a 95% CI for a standardised effect size ranging from 0.10 to 0.90 offers little clarity about the true effect. In contrast, a study with a larger sample that reports a 95% CI ranging from 0.5 to 0.6 provides a more precise estimate to the scientific literature.

Another consequence of studies with underpowered designs in a literature that selects for significance is a high false discovery risk [@button2013; @colquhoun2014]. Researchers typically set $\alpha$ to 0.05 to limit the long-term probability of making a type I error. However, setting $\alpha$ to 0.05 does not ensure that the literature will contain at most 5% of type I errors if there is selection bias in the literature. If researchers select a statistically significant study from a literature that suffers from selection bias and low power, the probability that this study is a type I error will be much higher than 5%. This is indeed what our analysis reveals: an average power of `r round(results$coefficients[2, 1], 2)*100`% results in a Maximum False Discovery Risk of `r round(results$coefficients[3, 1], 2)*100`% (95% CI \[`r round(results$coefficients[3, 2], 2)*100`; `r round(results$coefficients[3, 3], 2)*100`\]). The point estimate of `r round(results$coefficients[3, 1], 2)*100`% indicates a high risk of type I errors results where nearly every other significant result is a false positive. However, the range around this estimate is wide. The upper limit reaches 100%, suggesting that all significant results are false positives. While this is unlikely, the results suggest a high risk that significant results are false positives or true effects with negligible effect sizes. Researchers should be aware of the probability that findings in the literature can have a high average probability of being a type I error. To reduce the risk of type I error, it is essential to fully report all results regardless of statistical significance and to design studies with high power.

The second aim of this meta-study was to assess the presence of selection bias. Although our Observed Discovery Rate of `r round(primary_zcurve$N_sig/primary_zcurve$N_obs, 2)*100`% is not as high as the 81% estimate previously reported by Büttner et al. [-@buttner2020], there is a discrepancy of `r floor(((primary_zcurve$N_sig/primary_zcurve$N_obs)-results$coefficients[2, 3])*100)`% between the Observed Discovery Rate (`r round(primary_zcurve$N_sig/primary_zcurve$N_obs, 2)*100`%) and the upper bound of the Expected Discovery Rate 95% CI \[`r round(results$coefficients[2, 2], 2)*100`; `r round(results$coefficients[2, 3], 2)*100`\]. This gap indicates strong evidence of selection bias. The Expected Replication Rate was `r round(results$coefficients[1, 1], 2)*100`% indicating that only half of the studies that reported a significant effect would replicate. Our observed Expected Replication Rate of `r round(results$coefficients[1, 1], 2)*100`% (95% CI \[`r round(results$coefficients[1, 2], 2)*100`; `r round(results$coefficients[1, 3], 2)*100`\]) is in line with the actually observed 56% replication rate (based on statistical significance of the replication studies) observed in the sports science replication project [@murphy_replicability_2025]. Therefore, taken together, these findings present reasonable evidence of inflated type I error rates in our literature body. It is important to point out that the Expected Replication Rate is not a complement of the type I error rate. That is, a 50% Expected Replication Rate does not indicate that 50% of the replications would fail because the original findings were type I errors. Recall that the Expected Replication Rate is defined as the probability of obtaining a significant result using the original sample size in a replication study. Thus, a replication study could fail because the original study was a type I error, but also because its study design lacks the power to detect the true effect. While the exact contributions of type I and type II errors to the Expected Replication Rate remain unknown in our sample of studies, we can compare the Expected Replication Failure Rate (1 – Expected Replication Rate) with the Maximum False Discovery Risk to interpret replication failures. The Maximum False Discovery Risk (`r round(results$coefficients[3, 1], 2)*100`%) is close to the Expected Replication Failure Rate (`r (1 - round(results$coefficients[1, 1], 2))*100`%) suggesting that in the worst-case scenario almost half of the potential replication failures could be due to type I errors in original studies. Although it is desirable to be able to determine how many type I errors are published in the literature, our study is a stark reminder that without high-powered study designs and an unbiased literature, distinguishing between true and false findings becomes increasingly difficult. The best we can do is to urge researchers to consider the possibility that published studies might not replicate, even though the exact probability remains unknown.

The concept of the file drawer was introduced by Rosenthal [-@rosenthal1979] and refers to unpublished studies that produced non-significant results. If studies had 80% power, there would be only one non-significant study in the file drawer for every four published significant studies (File-Drawer Ratio = 1:4 or 0.25:1). However, if studies have only 20% power, there would be four non-significant studies for every published significant study (File-Drawer Ratio = 4:1). Thus, the impact of file-drawer bias increases as study power decreases. The file-drawer problem has important implications for accessing an unbiased literature. Underpowered studies can be combined in a meta-analysis to estimate the true effect size accurately, but this assumes that all effect sizes are published. When publication bias occurs—meaning studies that support their hypotheses are more likely to be published—some effect sizes remain unreported, which can lead to biased estimates of the true effect size.

Given the presence of selection bias and an average power of `r round(results$coefficients[2, 1], 2)*100`% in our sampled studies, statistically significant results in the literature are only possible with inflated effect sizes, where the true, unbiased effect size is actually much smaller—and possibly even zero—. It is therefore not surprising that a common finding among replication projects is that unbiased replication studies with larger sample sizes produce much smaller effect sizes [@murphy_replicability_2025; @errington2021; @osc2015]. For instance, the sport science replication project found that 88% of the original effect sizes were severely inflated in comparison to the replication effect sizes, with a median percentage decrease of 75% [@murphy_replicability_2025]. The goal of any empirical science should be the accumulation of reliable knowledge that researchers can build upon to develop new theories, formulate hypotheses, design experiments or conduct meta-analyses [@curran2009]. However, our findings in combination with those reported by the sports science replication project suggest that many studies published in our field are upwardly biased, hindering the notion of cumulative science.

To improve the informational value of studies published in the sports and exercise literature, the field should adopt several complementary practices to prevent selection bias and underpowered designs. First, Registered Reports are an effective safeguard against publication bias and *p*-hacking [@chambers_2021; @nosek_lakens_2014]. In this format, the study protocol—including hypotheses, methods, and statistical analyses—is peer reviewed before data collection, and journals offer in-principle acceptance, meaning the study will be published regardless of whether the hypotheses are supported, provided the approved protocol is followed. Despite their utility, only three journals in sport and exercise science currently accept Registered Reports [@abt_rr2021; @impellizzeri_2019; @rasmussen2025].

Researchers should also design their studies with high power by conducting rigorous a priori power analyses. Unfortunately, only 41% of studies in our sample performed an a priori power analysis to justify the sample size, and of those, many were poorly conducted [@mesquida_power_analysis_2025]. Combined with the systematic use of small samples [@Abt2020; @mesquida2022], this is a serious concern for the field. Journals should require valid sample size justifications [@lakenssamplejustification], researchers should ensure power analyses are conducted correctly, and collaborative research should be considered when individual data collection is challenging. Researchers should avoid overgeneralizing results from underpowered studies [@impellizzeri2025]. Beyond these practices, the field should increasingly adopt higher standard for open data and code [@borg_sharing_practices], transparently report exploratory research [@ditroilo], and collaborate with statisticians [@sainani_collaboration]. Together, these measures can substantially increase the reproducibility, replicability and therefore informational value of research in sports and exercise science.

We need to highlight three limitations of our study. First, even though we followed a coding scheme, the raters often had to make subjective decisions when selecting the key statistical result. These difficulties arose because hypotheses were often vaguely stated, mainly as a result of two issues: 1) the effect of interest was often not clearly stated, and 2) the primary outcome was often operationalized using additional measures of the same construct, or measured in multiple ways [@wicherts_2016]. These two issues, either in isolation or in combination, result in a multiplicity of hypothesis tests, which makes it difficult to link the tested hypothesis to the statistical result. Second, we included only studies that tested their hypotheses with *t*-tests or ANOVAs and thus excluded studies that used other types of statistical tests, such as mixed models or Bayesian analyses. We do not know if our results generalize to other designs or analyses. Third, `r excluded[1]` (`excluded[2]`) out of the `r nrow(zcurve_data)` independent *p*-values were excluded due to poor reporting practices or misuse of hypothesis tests (e.g., testing a hypothesis of no difference with a classic null hypothesis test). This means our findings do not generalize to studies that fail to fully report statistical results.

## Conclusion

Overall, our findings indicate that there is substantial selection bias in sports and exercise science. The average power of the sampled studies is `r round(results$coefficients[2, 1], 2)*100`% (95% CI \[`r round(results$coefficients[2, 2], 2)*100`; `r round(results$coefficients[2, 3], 2)*100`\]), and just one quarter of the studies seem to have been designed with high power. The presence of selection bias in combination with low average power is likely to contribute to a literature characterized by inflated effect sizes, a high proportion of type I and II errors, and therefore a low replicability rate. The $z$-curve analysis estimates that about half of the published significant findings might not replicate in a direct replication. Together, these findings should be a cause of concern for all researchers in the discipline. Sport and exercise science should make a collective effort to build a more informative and reliable knowledge base.

## References
